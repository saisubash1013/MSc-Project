{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisubash1013/MSc-Project/blob/main/H%26E_to_IHC_Project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dkw9Xg8C6nl"
      },
      "source": [
        "# Pix2Pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6iIqmV7-TkN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/HER2/TrainValAB\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxfSoKabC_C7"
      },
      "outputs": [],
      "source": [
        "# Cell 2a: Check drive mount and find dataset\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if the path exists\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "print(\"Contents of MyDrive:\")\n",
        "print(os.listdir(base_path))\n",
        "\n",
        "# Look for HER2 folder\n",
        "if 'HER2' in os.listdir(base_path):\n",
        "    print(\"\\nContents of HER2 folder:\")\n",
        "    print(os.listdir(f\"{base_path}/HER2\"))\n",
        "else:\n",
        "    print(\"\\nHER2 folder not found. Available folders:\")\n",
        "    for item in os.listdir(base_path):\n",
        "        if os.path.isdir(f\"{base_path}/{item}\"):\n",
        "            print(f\"  {item}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfr1bvXDDP7D"
      },
      "outputs": [],
      "source": [
        "# Cell 2b: Check dataset structure\n",
        "DATA_PATH = \"/content/drive/MyDrive/HER2/TrainValAB\"\n",
        "\n",
        "print(\"Contents of TrainValAB:\")\n",
        "print(os.listdir(DATA_PATH))\n",
        "\n",
        "# Check each folder\n",
        "for folder in ['trainA', 'trainB', 'valA', 'valB']:\n",
        "    folder_path = os.path.join(DATA_PATH, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        count = len(os.listdir(folder_path))\n",
        "        print(f\"{folder}: {count} images\")\n",
        "    else:\n",
        "        print(f\"{folder}: NOT FOUND\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503o7hVBEHFb"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Generator (U-Net)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(3, 64, normalize=False)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "        self.enc5 = self.conv_block(512, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = self.upconv_block(512, 512)\n",
        "        self.dec2 = self.upconv_block(1024, 256)\n",
        "        self.dec3 = self.upconv_block(512, 128)\n",
        "        self.dec4 = self.upconv_block(256, 64)\n",
        "        self.dec5 = nn.ConvTranspose2d(128, 3, 4, 2, 1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, normalize=True):\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d1 = self.dec1(e5)\n",
        "        d1 = torch.cat([d1, e4], dim=1)\n",
        "\n",
        "        d2 = self.dec2(d1)\n",
        "        d2 = torch.cat([d2, e3], dim=1)\n",
        "\n",
        "        d3 = self.dec3(d2)\n",
        "        d3 = torch.cat([d3, e2], dim=1)\n",
        "\n",
        "        d4 = self.dec4(d3)\n",
        "        d4 = torch.cat([d4, e1], dim=1)\n",
        "\n",
        "        output = torch.tanh(self.dec5(d4))\n",
        "        return output\n",
        "\n",
        "generator = Generator().to(device)\n",
        "print(\"Generator created\")\n",
        "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--LQJCyESrV"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Discriminator (PatchGAN)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 4, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img, target_img):\n",
        "        x = torch.cat([input_img, target_img], dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "print(\"Discriminator created\")\n",
        "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly3W4QSJEeSX"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Training Setup\n",
        "criterion_GAN = nn.BCELoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 10  # Start with 10 epochs\n",
        "lambda_L1 = 100\n",
        "start_epoch = 0\n",
        "\n",
        "# Create checkpoint directory\n",
        "os.makedirs('/content/drive/MyDrive/checkpointsPix2Pix', exist_ok=True)\n",
        "\n",
        "print(\"Training setup complete\")\n",
        "print(f\"Will train for {num_epochs} epochs\")\n",
        "print(f\"L1 loss weight: {lambda_L1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSUijSjiEzDC"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Resume Training Function\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    print(f\"Resumed from epoch {epoch}\")\n",
        "    return epoch\n",
        "\n",
        "# Uncomment to resume training from specific epoch\n",
        "# start_epoch = load_checkpoint('/content/drive/MyDrive/checkpointsPix2Pix/checkpoint_epoch_5.pth')\n",
        "\n",
        "print(\"Resume function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJh4vxtFJnI"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Training Loop with Checkpoints\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "training_losses = {'g_losses': [], 'd_losses': []}\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    epoch_g_loss = 0\n",
        "    epoch_d_loss = 0\n",
        "\n",
        "    for i, (he_images, ihc_images) in enumerate(train_loader):\n",
        "        he_images = he_images.to(device)\n",
        "        ihc_images = ihc_images.to(device)\n",
        "\n",
        "        batch_size = he_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1, 30, 30).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, 30, 30).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_output = discriminator(he_images, ihc_images)\n",
        "        d_real_loss = criterion_GAN(real_output, real_labels)\n",
        "\n",
        "        fake_ihc = generator(he_images)\n",
        "        fake_output = discriminator(he_images, fake_ihc.detach())\n",
        "        d_fake_loss = criterion_GAN(fake_output, fake_labels)\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) * 0.5\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        fake_output = discriminator(he_images, fake_ihc)\n",
        "        g_gan_loss = criterion_GAN(fake_output, real_labels)\n",
        "        g_l1_loss = criterion_L1(fake_ihc, ihc_images)\n",
        "\n",
        "        g_loss = g_gan_loss + lambda_L1 * g_l1_loss\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        epoch_g_loss += g_loss.item()\n",
        "        epoch_d_loss += d_loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], '\n",
        "                  f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_g_loss = epoch_g_loss / len(train_loader)\n",
        "    avg_d_loss = epoch_d_loss / len(train_loader)\n",
        "\n",
        "    training_losses['g_losses'].append(avg_g_loss)\n",
        "    training_losses['d_losses'].append(avg_d_loss)\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        'g_loss': avg_g_loss,\n",
        "        'd_loss': avg_d_loss,\n",
        "        'training_losses': training_losses\n",
        "    }\n",
        "    torch.save(checkpoint, f'/content/drive/MyDrive/checkpointsPix2Pix/checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f} - SAVED')\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: Full evaluation (auto-detects Pix2Pix vs Cycle U-Net), paired metrics + FID ---\n",
        "import os, glob, csv, time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from scipy import linalg\n",
        "\n",
        "# -------- use paths from STEP 2 --------\n",
        "assert 'CHECKPOINT_PATH' in globals() and os.path.exists(CHECKPOINT_PATH), \"Checkpoint not found. Re-run Step 2.\"\n",
        "assert 'VALA_DIR' in globals() and 'VALB_DIR' in globals() and os.path.exists(VALA_DIR) and os.path.exists(VALB_DIR), \"valA/valB not found. Re-run Step 2.\"\n",
        "\n",
        "ROOT = os.path.dirname(os.path.dirname(CHECKPOINT_PATH))  # .../HER2\n",
        "EVAL_DIR = os.path.join(ROOT, \"pix2pix_eval\", \"epoch_200_autodetect\")\n",
        "FAKE_DIR = os.path.join(EVAL_DIR, \"fakeIHC\")\n",
        "TRIP_DIR = os.path.join(EVAL_DIR, \"triptychs_10\")\n",
        "os.makedirs(FAKE_DIR, exist_ok=True)\n",
        "os.makedirs(TRIP_DIR, exist_ok=True)\n",
        "\n",
        "CSV_PATH = os.path.join(EVAL_DIR, \"metrics_per_image.csv\")\n",
        "SUMMARY_PATH = os.path.join(EVAL_DIR, \"metrics_summary.txt\")\n",
        "\n",
        "N_EVAL = 1000           # evaluate up to this many pairs\n",
        "SAVE_TRIPTYCHS = True   # save 10 H&E|Real|Fake triptychs\n",
        "COMPUTE_FID = True      # set-level realism\n",
        "FID_BATCH = 32\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Using checkpoint:\", CHECKPOINT_PATH)\n",
        "\n",
        "# ----- Generators -----\n",
        "class Pix2PixUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1 = self._c(3,64,False); self.enc2 = self._c(64,128); self.enc3 = self._c(128,256)\n",
        "        self.enc4 = self._c(256,512); self.enc5 = self._c(512,512)\n",
        "        self.dec1 = self._u(512,512); self.dec2 = self._u(1024,256)\n",
        "        self.dec3 = self._u(512,128); self.dec4 = self._u(256,64)\n",
        "        self.dec5 = nn.ConvTranspose2d(128,3,4,2,1)\n",
        "    def _c(self,i,o,norm=True):\n",
        "        layers=[nn.Conv2d(i,o,4,2,1)]\n",
        "        if norm: layers.append(nn.BatchNorm2d(o))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        return nn.Sequential(*layers)\n",
        "    def _u(self,i,o):\n",
        "        return nn.Sequential(nn.ConvTranspose2d(i,o,4,2,1), nn.BatchNorm2d(o), nn.ReLU())\n",
        "    def forward(self,x):\n",
        "        e1=self.enc1(x); e2=self.enc2(e1); e3=self.enc3(e2); e4=self.enc4(e3); e5=self.enc5(e4)\n",
        "        d1=self.dec1(e5); d1=torch.cat([d1,e4],1)\n",
        "        d2=self.dec2(d1); d2=torch.cat([d2,e3],1)\n",
        "        d3=self.dec3(d2); d3=torch.cat([d3,e2],1)\n",
        "        d4=self.dec4(d3); d4=torch.cat([d4,e1],1)\n",
        "        return torch.tanh(self.dec5(d4))\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, down=True, act=\"relu\", use_bn=True):\n",
        "        super().__init__()\n",
        "        norm = nn.InstanceNorm2d(out_ch) if use_bn else nn.Identity()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False, padding_mode=\"reflect\") if down\n",
        "            else nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "            norm,\n",
        "            nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    def forward(self,x): return self.conv(x)\n",
        "\n",
        "class CycleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, feat=64):\n",
        "        super().__init__()\n",
        "        self.initial_down = ConvBlock(in_ch, feat, True, \"leaky\", False)\n",
        "        self.down1=ConvBlock(feat,feat*2,True,\"leaky\")\n",
        "        self.down2=ConvBlock(feat*2,feat*4,True,\"leaky\")\n",
        "        self.down3=ConvBlock(feat*4,feat*8,True,\"leaky\")\n",
        "        self.down4=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.down5=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.down6=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.bottleneck=nn.Sequential(nn.Conv2d(feat*8,feat*8,4,2,1,padding_mode=\"reflect\"), nn.ReLU())\n",
        "        self.up1=ConvBlock(feat*8,feat*8,False,\"relu\",True)\n",
        "        self.up2=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up3=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up4=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up5=ConvBlock(feat*16,feat*4,False,\"relu\",True)\n",
        "        self.up6=ConvBlock(feat*8,feat*2,False,\"relu\",True)\n",
        "        self.up7=ConvBlock(feat*4,feat,False,\"relu\",True)\n",
        "        self.final_up=nn.Sequential(nn.ConvTranspose2d(feat*2,out_ch,4,2,1), nn.Tanh())\n",
        "    def forward(self,x):\n",
        "        d1=self.initial_down(x); d2=self.down1(d1); d3=self.down2(d2)\n",
        "        d4=self.down3(d3); d5=self.down4(d4); d6=self.down5(d5); d7=self.down6(d6)\n",
        "        b=self.bottleneck(d7)\n",
        "        u1=self.up1(b); u2=self.up2(torch.cat([u1,d7],1))\n",
        "        u3=self.up3(torch.cat([u2,d6],1)); u4=self.up4(torch.cat([u3,d5],1))\n",
        "        u5=self.up5(torch.cat([u4,d4],1)); u6=self.up6(torch.cat([u5,d3],1))\n",
        "        u7=self.up7(torch.cat([u6,d2],1))\n",
        "        return self.final_up(torch.cat([u7,d1],1))\n",
        "\n",
        "# ----- Load checkpoint & auto-detect arch -----\n",
        "def load_state_dict_from_checkpoint(path, device):\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    if isinstance(ckpt, dict):\n",
        "        for k in [\"generator_state_dict\",\"gen_A_state_dict\",\"state_dict\",\"model\",\"netG\",\"G\"]:\n",
        "            if k in ckpt and isinstance(ckpt[k], dict):\n",
        "                sd = ckpt[k]; break\n",
        "        else:\n",
        "            if all(torch.is_tensor(v) for v in ckpt.values()):\n",
        "                sd = ckpt\n",
        "            else:\n",
        "                raise RuntimeError(f\"Unrecognized checkpoint keys: {list(ckpt.keys())[:8]}\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Checkpoint must be a dict/state_dict.\")\n",
        "    first = next(iter(sd))\n",
        "    if first.startswith(\"module.\"):\n",
        "        sd = {k.replace(\"module.\",\"\",1): v for k,v in sd.items()}\n",
        "    return sd\n",
        "\n",
        "def detect_arch(sd_keys):\n",
        "    ks = list(sd_keys)\n",
        "    if any(k.startswith(\"enc1\") or \".enc1.\" in k for k in ks): return \"pix2pix\"\n",
        "    if any(k.startswith(\"initial_down\") or \".initial_down.\" in k for k in ks): return \"cyc_unet\"\n",
        "    if any(\".down1.\" in k for k in ks): return \"cyc_unet\"\n",
        "    return \"unknown\"\n",
        "\n",
        "sd = load_state_dict_from_checkpoint(CHECKPOINT_PATH, DEVICE)\n",
        "arch = detect_arch(sd.keys())\n",
        "print(\"Detected checkpoint architecture:\", arch)\n",
        "\n",
        "if arch == \"pix2pix\":\n",
        "    G = Pix2PixUNet().to(DEVICE).eval()\n",
        "elif arch == \"cyc_unet\":\n",
        "    G = CycleUNet().to(DEVICE).eval()\n",
        "else:\n",
        "    raise RuntimeError(\"Could not detect architecture from checkpoint keys.\")\n",
        "G.load_state_dict(sd, strict=False)\n",
        "\n",
        "# ----- Pair files by basename -----\n",
        "def index_by_base(folder):\n",
        "    idx = {}\n",
        "    for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.bmp\",\"*.webp\"):\n",
        "        for p in glob.glob(os.path.join(folder, ext)):\n",
        "            idx[os.path.splitext(os.path.basename(p))[0]] = p\n",
        "    return idx\n",
        "\n",
        "A_idx = index_by_base(VALA_DIR)   # H&E\n",
        "B_idx = index_by_base(VALB_DIR)   # real IHC\n",
        "common = sorted(set(A_idx) & set(B_idx))\n",
        "assert common, \"No matching basenames between valA and valB.\"\n",
        "common = common[:N_EVAL]\n",
        "print(f\"Evaluating {len(common)} paired tiles.\")\n",
        "\n",
        "# ----- Transforms -----\n",
        "tx_in = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
        "])\n",
        "tx_01 = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor(),  # [0,1]\n",
        "])\n",
        "to01 = lambda t: (t*0.5 + 0.5).clamp(0,1)\n",
        "to_pil = transforms.ToPILImage()\n",
        "def tnp01(t): return t.permute(1,2,0).cpu().numpy().astype(np.float32)\n",
        "\n",
        "# ----- Optional FID setup -----\n",
        "if COMPUTE_FID:\n",
        "    weights = Inception_V3_Weights.IMAGENET1K_V1\n",
        "    inception = inception_v3(weights=weights).to(DEVICE).eval()\n",
        "    inception.fc = nn.Identity()\n",
        "    pre_inception = weights.transforms()\n",
        "    gen_pils, real_pils = [], []\n",
        "\n",
        "def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
        "    diff = mu1 - mu2\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2*np.trace(covmean)\n",
        "\n",
        "# ----- Evaluate -----\n",
        "rows = []\n",
        "trip_bases = common[:10] if SAVE_TRIPTYCHS else []\n",
        "t0 = time.time()\n",
        "\n",
        "with torch.no_grad(), open(CSV_PATH, \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"basename\",\"ssim\",\"psnr\",\"mae\",\"mse\"])\n",
        "\n",
        "    for base in tqdm(common, desc=\"Evaluating\"):\n",
        "        he_p, ihc_p = A_idx[base], B_idx[base]\n",
        "        he_img = Image.open(he_p).convert(\"RGB\")\n",
        "        ihc_img = Image.open(ihc_p).convert(\"RGB\")\n",
        "\n",
        "        x = tx_in(he_img).unsqueeze(0).to(DEVICE)  # [-1,1]\n",
        "        fake = G(x)                                # [-1,1]\n",
        "        fake01 = to01(fake)[0]                     # [C,H,W] on GPU\n",
        "        fake01_cpu = fake01.detach().cpu()         # move to CPU for saving/concat\n",
        "\n",
        "        real01 = tx_01(ihc_img)                    # [C,H,W] CPU\n",
        "\n",
        "        # Save generated\n",
        "        save_image(fake01_cpu, os.path.join(FAKE_DIR, base + \"_fakeIHC.png\"))\n",
        "\n",
        "        # Metrics on [0,1]\n",
        "        g_np = tnp01(fake01_cpu)\n",
        "        r_np = tnp01(real01)\n",
        "        s = ssim(r_np, g_np, data_range=1.0, channel_axis=2)\n",
        "        p = psnr(r_np, g_np, data_range=1.0)\n",
        "        a = float(np.abs(r_np - g_np).mean())\n",
        "        m = float(((r_np - g_np)**2).mean())\n",
        "        writer.writerow([base, f\"{s:.6f}\", f\"{p:.6f}\", f\"{a:.6f}\", f\"{m:.6f}\"])\n",
        "        rows.append((s,p,a,m))\n",
        "\n",
        "        # Triptych (CPU tensors only)\n",
        "        if base in trip_bases:\n",
        "            he01 = tx_01(he_img)                  # CPU\n",
        "            trip = torch.cat([he01, real01, fake01_cpu], dim=2)\n",
        "            save_image(trip, os.path.join(TRIP_DIR, base + \"_TRIPTYCH.png\"))\n",
        "\n",
        "        # FID prep (store PILs)\n",
        "        if COMPUTE_FID:\n",
        "            gen_pils.append(to_pil(fake01_cpu))\n",
        "            real_pils.append(ihc_img)\n",
        "\n",
        "# ----- Summary -----\n",
        "import statistics as stats\n",
        "S=[r[0] for r in rows]; P=[r[1] for r in rows]; A=[r[2] for r in rows]; M=[r[3] for r in rows]\n",
        "summary = [\n",
        "    f\"Detected arch: {arch}\",\n",
        "    f\"Checkpoint: {os.path.basename(CHECKPOINT_PATH)}\",\n",
        "    f\"N evaluated: {len(rows)}\",\n",
        "    f\"SSIM: mean={stats.mean(S):.4f}  median={stats.median(S):.4f}  std={stats.pstdev(S):.4f}\",\n",
        "    f\"PSNR: mean={stats.mean(P):.2f} dB  median={stats.median(P):.2f} dB  std={stats.pstdev(P):.2f}\",\n",
        "    f\"MAE:  mean={stats.mean(A):.4f}\",\n",
        "    f\"MSE:  mean={stats.mean(M):.5f}\",\n",
        "]\n",
        "\n",
        "# ----- FID -----\n",
        "if COMPUTE_FID:\n",
        "    def feats_from_pils(pils, bs=FID_BATCH):\n",
        "        feats=[]\n",
        "        for i in tqdm(range(0, len(pils), bs), desc=\"Inception features\"):\n",
        "            xb = torch.stack([pre_inception(im) for im in pils[i:i+bs]], 0).to(DEVICE)\n",
        "            with torch.no_grad(): feats.append(inception(xb).cpu().numpy())\n",
        "        return np.concatenate(feats, 0)\n",
        "\n",
        "    fake_feats = feats_from_pils(gen_pils, bs=FID_BATCH)\n",
        "    real_feats = feats_from_pils(real_pils, bs=FID_BATCH)\n",
        "    mu_f, sigma_f = fake_feats.mean(0), np.cov(fake_feats, rowvar=False)\n",
        "    mu_r, sigma_r = real_feats.mean(0), np.cov(real_feats, rowvar=False)\n",
        "    fid = float(frechet_distance(mu_f, sigma_f, mu_r, sigma_r))\n",
        "    summary.append(f\"FID: {fid:.4f}\")\n",
        "\n",
        "summary.append(f\"Time: {(time.time()-t0)/60:.1f} min\")\n",
        "print(\"\\n\" + \"\\n\".join(summary))\n",
        "\n",
        "with open(SUMMARY_PATH, \"w\") as f:\n",
        "    f.write(\"\\n\".join(summary))\n",
        "\n",
        "print(\"\\nPer-image CSV:\", CSV_PATH)\n",
        "print(\"Summary text :\", SUMMARY_PATH)\n",
        "print(\"Fakes saved  :\", FAKE_DIR)\n",
        "if SAVE_TRIPTYCHS:\n",
        "    print(\"Triptychs (10):\", TRIP_DIR)\n"
      ],
      "metadata": {
        "id": "ODDRqXuhqj4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Add-on: Compute KID × 1000 for the Pix2Pix/Cycle-UNet eval using saved fakes ---\n",
        "# Uses images already in FAKE_DIR and real IHCs in VALB_DIR.\n",
        "import os, glob, math, random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "# Reuse these from STEP 3; adjust if needed\n",
        "assert 'FAKE_DIR' in globals() and os.path.exists(FAKE_DIR), \"FAKE_DIR missing; run STEP 3 first.\"\n",
        "assert 'VALB_DIR' in globals() and os.path.exists(VALB_DIR), \"VALB_DIR missing.\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"FAKE_DIR:\", FAKE_DIR)\n",
        "print(\"VALB_DIR:\", VALB_DIR)\n",
        "\n",
        "# Index reals by basename\n",
        "def index_by_base(folder):\n",
        "    idx = {}\n",
        "    for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.bmp\",\"*.webp\"):\n",
        "        for p in glob.glob(os.path.join(folder, ext)):\n",
        "            idx[os.path.splitext(os.path.basename(p))[0]] = p\n",
        "    return idx\n",
        "\n",
        "real_idx = index_by_base(VALB_DIR)\n",
        "\n",
        "# Collect matching pairs from FAKE_DIR\n",
        "fake_paths = sorted(glob.glob(os.path.join(FAKE_DIR, \"*_fakeIHC.png\")))\n",
        "pairs = []\n",
        "for fp in fake_paths:\n",
        "    base = os.path.basename(fp).replace(\"_fakeIHC.png\", \"\")\n",
        "    if base in real_idx:\n",
        "        pairs.append((fp, real_idx[base]))\n",
        "print(f\"Found {len(pairs)} fake-vs-real pairs for KID.\")\n",
        "\n",
        "# Inception feature extractor\n",
        "weights = Inception_V3_Weights.IMAGENET1K_V1\n",
        "preproc = weights.transforms()\n",
        "inception = inception_v3(weights=weights).to(DEVICE).eval()\n",
        "inception.fc = nn.Identity()  # 2048-d features\n",
        "\n",
        "def feats_from_pairs(pairs, which=\"fake\", bs=32):\n",
        "    imgs = [Image.open(fp if which==\"fake\" else rp).convert(\"RGB\") for fp, rp in pairs]\n",
        "    F = []\n",
        "    for i in tqdm(range(0, len(imgs), bs), desc=f\"Inception features ({which})\"):\n",
        "        xb = torch.stack([preproc(im) for im in imgs[i:i+bs]], 0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            f = inception(xb).cpu().numpy()\n",
        "        F.append(f)\n",
        "    return np.concatenate(F, 0)\n",
        "\n",
        "fake_feats = feats_from_pairs(pairs, \"fake\", bs=32)\n",
        "real_feats = feats_from_pairs(pairs, \"real\", bs=32)\n",
        "print(\"Feature shapes:\", fake_feats.shape, real_feats.shape)\n",
        "\n",
        "# --- KID (unbiased MMD^2 with polynomial kernel (x^T y / d + 1)^3), averaged over splits ---\n",
        "def polynomial_mmd2_unbiased(X, Y, degree=3, gamma=None, coef0=1.0):\n",
        "    # X, Y: [n, d]\n",
        "    n = X.shape[0]\n",
        "    d = X.shape[1]\n",
        "    if gamma is None:\n",
        "        gamma = 1.0 / d\n",
        "    # shuffle to avoid diagonal bias, then use off-diagonals for unbiased estimate\n",
        "    idx = np.random.permutation(n)\n",
        "    X = X[idx]\n",
        "    Y = Y[idx]\n",
        "    XX = (X @ X.T) * gamma + coef0\n",
        "    YY = (Y @ Y.T) * gamma + coef0\n",
        "    XY = (X @ Y.T) * gamma + coef0\n",
        "    XX = XX**degree\n",
        "    YY = YY**degree\n",
        "    XY = XY**degree\n",
        "    # Unbiased MMD^2: exclude diagonals for XX and YY\n",
        "    np.fill_diagonal(XX, 0.0)\n",
        "    np.fill_diagonal(YY, 0.0)\n",
        "    mmd2 = XX.sum()/(n*(n-1)) + YY.sum()/(n*(n-1)) - 2.0*XY.mean()\n",
        "    return float(mmd2)\n",
        "\n",
        "def compute_kid(feats_fake, feats_real, n_splits=10, max_subset=1000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = min(len(feats_fake), len(feats_real), max_subset)\n",
        "    scores = []\n",
        "    for _ in range(n_splits):\n",
        "        idx_f = rng.choice(len(feats_fake), n, replace=False)\n",
        "        idx_r = rng.choice(len(feats_real), n, replace=False)\n",
        "        scores.append(polynomial_mmd2_unbiased(feats_fake[idx_f], feats_real[idx_r]))\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "kid_mean, kid_std = compute_kid(fake_feats, real_feats, n_splits=10, max_subset=1000, seed=123)\n",
        "print(f\"\\nKID × 1000: {kid_mean*1000:.2f} ± {kid_std*1000:.2f}\")\n"
      ],
      "metadata": {
        "id": "Z4Pjv6txZubH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Generate 10 triptychs from the given epoch-200 checkpoint ===\n",
        "import os, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/HER2/pix2pix_checkpoints/generator_epoch_200.pth\"\n",
        "VALA_DIR = \"/content/drive/MyDrive/HER2/TrainValAB/valA\"  # H&E\n",
        "VALB_DIR = \"/content/drive/MyDrive/HER2/TrainValAB/valB\"  # IHC\n",
        "OUT_DIR  = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "assert os.path.exists(CHECKPOINT_PATH), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
        "assert os.path.exists(VALA_DIR) and os.path.exists(VALB_DIR), \"valA/valB paths not found.\"\n",
        "\n",
        "# ---- Model defs ----\n",
        "class Pix2PixUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1=self._c(3,64,False); self.enc2=self._c(64,128); self.enc3=self._c(128,256)\n",
        "        self.enc4=self._c(256,512); self.enc5=self._c(512,512)\n",
        "        self.dec1=self._u(512,512); self.dec2=self._u(1024,256)\n",
        "        self.dec3=self._u(512,128); self.dec4=self._u(256,64)\n",
        "        self.dec5=nn.ConvTranspose2d(128,3,4,2,1)\n",
        "    def _c(self,i,o,norm=True):\n",
        "        layers=[nn.Conv2d(i,o,4,2,1)]\n",
        "        if norm: layers.append(nn.BatchNorm2d(o))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        return nn.Sequential(*layers)\n",
        "    def _u(self,i,o):\n",
        "        return nn.Sequential(nn.ConvTranspose2d(i,o,4,2,1), nn.BatchNorm2d(o), nn.ReLU())\n",
        "    def forward(self,x):\n",
        "        e1=self.enc1(x); e2=self.enc2(e1); e3=self.enc3(e2); e4=self.enc4(e3); e5=self.enc5(e4)\n",
        "        d1=self.dec1(e5); d1=torch.cat([d1,e4],1)\n",
        "        d2=self.dec2(d1); d2=torch.cat([d2,e3],1)\n",
        "        d3=self.dec3(d2); d3=torch.cat([d3,e2],1)\n",
        "        d4=self.dec4(d3); d4=torch.cat([d4,e1],1)\n",
        "        return torch.tanh(self.dec5(d4))\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, down=True, act=\"relu\", use_bn=True):\n",
        "        super().__init__()\n",
        "        norm = nn.InstanceNorm2d(out_ch) if use_bn else nn.Identity()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False, padding_mode=\"reflect\") if down\n",
        "            else nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "            norm, nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    def forward(self,x): return self.conv(x)\n",
        "\n",
        "class CycleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, feat=64):\n",
        "        super().__init__()\n",
        "        self.initial_down=ConvBlock(in_ch,feat,True,\"leaky\",False)\n",
        "        self.down1=ConvBlock(feat,feat*2,True,\"leaky\"); self.down2=ConvBlock(feat*2,feat*4,True,\"leaky\")\n",
        "        self.down3=ConvBlock(feat*4,feat*8,True,\"leaky\"); self.down4=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.down5=ConvBlock(feat*8,feat*8,True,\"leaky\"); self.down6=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.bottleneck=nn.Sequential(nn.Conv2d(feat*8,feat*8,4,2,1,padding_mode=\"reflect\"), nn.ReLU())\n",
        "        self.up1=ConvBlock(feat*8,feat*8,False,\"relu\",True)\n",
        "        self.up2=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up3=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up4=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up5=ConvBlock(feat*16,feat*4,False,\"relu\",True)\n",
        "        self.up6=ConvBlock(feat*8,feat*2,False,\"relu\",True)\n",
        "        self.up7=ConvBlock(feat*4,feat,False,\"relu\",True)\n",
        "        self.final_up=nn.Sequential(nn.ConvTranspose2d(feat*2,out_ch,4,2,1), nn.Tanh())\n",
        "    def forward(self,x):\n",
        "        d1=self.initial_down(x); d2=self.down1(d1); d3=self.down2(d2)\n",
        "        d4=self.down3(d3); d5=self.down4(d4); d6=self.down5(d5); d7=self.down6(d6)\n",
        "        b=self.bottleneck(d7)\n",
        "        u1=self.up1(b); u2=self.up2(torch.cat([u1,d7],1))\n",
        "        u3=self.up3(torch.cat([u2,d6],1)); u4=self.up4(torch.cat([u3,d5],1))\n",
        "        u5=self.up5(torch.cat([u4,d4],1)); u6=self.up6(torch.cat([u5,d3],1))\n",
        "        u7=self.up7(torch.cat([u6,d2],1))\n",
        "        return self.final_up(torch.cat([u7,d1],1))\n",
        "\n",
        "# ---- load checkpoint and detect arch ----\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "sd = None\n",
        "if isinstance(ckpt, dict):\n",
        "    for k in [\"generator_state_dict\",\"gen_A_state_dict\",\"state_dict\",\"model\",\"netG\",\"G\"]:\n",
        "        if k in ckpt and isinstance(ckpt[k], dict):\n",
        "            sd = ckpt[k]; break\n",
        "    if sd is None and all(torch.is_tensor(v) for v in ckpt.values()):\n",
        "        sd = ckpt\n",
        "else:\n",
        "    raise RuntimeError(\"Unexpected checkpoint format.\")\n",
        "first = next(iter(sd))\n",
        "if first.startswith(\"module.\"):\n",
        "    sd = {k.replace(\"module.\",\"\",1): v for k,v in sd.items()}\n",
        "\n",
        "def detect_arch(keys):\n",
        "    ks = list(keys)\n",
        "    if any(k.startswith(\"enc1\") or \".enc1.\" in k for k in ks): return \"pix2pix\"\n",
        "    if any(k.startswith(\"initial_down\") or \".initial_down.\" in k for k in ks): return \"cyc_unet\"\n",
        "    if any(\".down1.\" in k for k in ks): return \"cyc_unet\"\n",
        "    return \"unknown\"\n",
        "\n",
        "arch = detect_arch(sd.keys())\n",
        "print(\"Detected arch:\", arch)\n",
        "\n",
        "G = Pix2PixUNet().to(DEVICE).eval() if arch==\"pix2pix\" else CycleUNet().to(DEVICE).eval()\n",
        "G.load_state_dict(sd, strict=False)\n",
        "\n",
        "# ---- pair 10 filenames by basename ----\n",
        "def index_by_base(folder):\n",
        "    idx = {}\n",
        "    for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.bmp\",\"*.webp\"):\n",
        "        for p in glob.glob(os.path.join(folder, ext)):\n",
        "            idx[os.path.splitext(os.path.basename(p))[0]] = p\n",
        "    return idx\n",
        "\n",
        "A = index_by_base(VALA_DIR); B = index_by_base(VALB_DIR)\n",
        "common = sorted(set(A) & set(B))[:10]\n",
        "assert common, \"No matching basenames between valA and valB.\"\n",
        "\n",
        "# ---- transforms ----\n",
        "tx_in = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
        "])\n",
        "tx_01 = transforms.Compose([transforms.Resize(256), transforms.ToTensor()])\n",
        "to01 = lambda t: (t*0.5 + 0.5).clamp(0,1)\n",
        "\n",
        "# ---- generate ----\n",
        "print(\"Saving 10 triptychs to:\", OUT_DIR)\n",
        "with torch.no_grad():\n",
        "    for base in common:\n",
        "        he_img  = Image.open(A[base]).convert(\"RGB\")\n",
        "        ihc_img = Image.open(B[base]).convert(\"RGB\")\n",
        "        x = tx_in(he_img).unsqueeze(0).to(DEVICE)\n",
        "        fake = G(x)\n",
        "        fake01 = to01(fake)[0].cpu()\n",
        "        he01   = tx_01(he_img)\n",
        "        real01 = tx_01(ihc_img)\n",
        "        trip = torch.cat([he01, real01, fake01], dim=2)\n",
        "        save_image(trip,   os.path.join(OUT_DIR, f\"{base}_TRIPTYCH_e200.png\"))\n",
        "        save_image(fake01, os.path.join(OUT_DIR, f\"{base}_fakeIHC_e200.png\"))\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "K_xYqS0DbbXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the 10 saved triptychs (H&E | Real IHC | Generated IHC)\n",
        "import os, glob, math\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "\n",
        "trip_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"*_TRIPTYCH_e200.png\")))\n",
        "print(f\"Found {len(trip_paths)} triptychs in:\", OUT_DIR)\n",
        "assert trip_paths, f\"No triptych images found in {OUT_DIR}. Check that the previous generation step ran.\"\n",
        "\n",
        "cols = 5\n",
        "rows = math.ceil(len(trip_paths) / cols)\n",
        "plt.figure(figsize=(cols*4, rows*4))\n",
        "for i, p in enumerate(trip_paths[:rows*cols]):\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    ax = plt.subplot(rows, cols, i+1)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(os.path.basename(p), fontsize=8)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XI-pP2KCehib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "trip_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"*_TRIPTYCH_e200.png\")))\n",
        "p = random.choice(trip_paths)  # or set p to a specific file path\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(Image.open(p).convert(\"RGB\"))\n",
        "plt.title(os.path.basename(p))\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FCOZYgE5eoIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyuV_rrC9KXb"
      },
      "source": [
        "# **CycleGAN after pix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s00q4Lt6Eqiu"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Global Configuration\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.autograd as autograd # For gradient penalty\n",
        "# Updated AMP syntax for both GradScaler and autocast\n",
        "from torch.amp import GradScaler, autocast # Use torch.amp for both\n",
        "from torchvision.models import vgg19 # For Perceptual Loss\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Dataset paths\n",
        "# Although CycleGAN can work with unpaired data, we'll use your existing\n",
        "# paired folder structure for convenience. CycleGAN's loss will handle\n",
        "# the 'unpaired' nature internally.\n",
        "TRAIN_DIR_A = '/content/drive/MyDrive/HER2/TrainValAB/trainA' # H&E training images\n",
        "TRAIN_DIR_B = '/content/drive/MyDrive/HER2/TrainValAB/trainB' # IHC training images\n",
        "VAL_DIR_A = '/content/drive/MyDrive/HER2/TrainValAB/valA'     # H&E validation images\n",
        "VAL_DIR_B = '/content/drive/MyDrive/HER2/TrainValAB/valB'     # IHC validation images\n",
        "\n",
        "# Checkpoint and sample image saving paths for CycleGAN\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/HER2/cyclegan_checkpoints'\n",
        "SAMPLE_IMAGES_DIR = '/content/drive/MyDrive/HER2/cyclegan_sample_images'\n",
        "TRAINING_LOG_PATH = os.path.join(CHECKPOINT_DIR, 'cyclegan_training_loss_history.csv')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(SAMPLE_IMAGES_DIR, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE_GEN = 0.00005 # Adjusted: Further reduced Generator LR for WGAN-GP stability\n",
        "LEARNING_RATE_DISC = 0.000005 # Adjusted: Further reduced Discriminator LR for WGAN-GP stability\n",
        "BATCH_SIZE = 1 # CycleGAN often uses batch size 1\n",
        "NUM_EPOCHS = 200\n",
        "LOAD_MODEL = True # Set to False for a fresh start as requested\n",
        "SAVE_MODEL = True\n",
        "IMAGE_SIZE = 256 # Images will be resized to this resolution\n",
        "\n",
        "# Loss weights for CycleGAN\n",
        "LAMBDA_CYCLE = 10.0 # Weight for cycle consistency loss (crucial for CycleGAN)\n",
        "LAMBDA_IDENTITY = 0.5 * LAMBDA_CYCLE # Weight for identity mapping loss (prevents color shifts)\n",
        "LAMBDA_GP = 10.0 # Weight for Gradient Penalty in WGAN-GP\n",
        "LAMBDA_PERCEPTUAL = 0.01 # Adjusted: Reduced Perceptual Loss weight to prevent instability\n",
        "\n",
        "# Learning Rate Decay parameters\n",
        "DECAY_EPOCH_START = 100 # Epoch to start linear decay of learning rates\n",
        "\n",
        "# Image transformations\n",
        "# Note: CycleGAN often uses a slightly different transform set than Pix2Pix\n",
        "# For simplicity, we'll keep the same transforms as your previous setup.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "print(\"Global configuration loaded for CycleGAN.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p_6dPiOGxsuN"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Dataset Class and DataLoader\n",
        "\n",
        "class UnpairedImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_A, root_B, transform=None):\n",
        "        self.root_A = root_A # Images for Domain A (e.g., H&E)\n",
        "        self.root_B = root_B # Images for Domain B (e.g., IHC)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images_A = sorted(os.listdir(root_A))\n",
        "        self.images_B = sorted(os.listdir(root_B))\n",
        "\n",
        "        # CycleGAN doesn't require paired images, so we just take the minimum length\n",
        "        # to ensure we don't run out of images for one domain before the other.\n",
        "        self.length_dataset = max(len(self.images_A), len(self.images_B))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Cycle through images if one domain has fewer images than the other\n",
        "        img_A_path = os.path.join(self.root_A, self.images_A[index % len(self.images_A)])\n",
        "        img_B_path = os.path.join(self.root_B, self.images_B[index % len(self.images_B)])\n",
        "\n",
        "        img_A = Image.open(img_A_path).convert(\"RGB\")\n",
        "        img_B = Image.open(img_B_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img_A = self.transform(img_A)\n",
        "            img_B = self.transform(img_B)\n",
        "\n",
        "        return img_A, img_B\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = UnpairedImageDataset(TRAIN_DIR_A, TRAIN_DIR_B, transform=transform)\n",
        "val_dataset = UnpairedImageDataset(VAL_DIR_A, VAL_DIR_B, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Dataset and DataLoaders created for CycleGAN (handling unpaired data).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lIGGI4ezO1d"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Model Definitions (Generator and Discriminator for CycleGAN)\n",
        "\n",
        "# --- Generator Model (U-Net Architecture - Copied from your successful Pix2Pix inference) ---\n",
        "# This is the Generator architecture that successfully loaded your previous checkpoint.\n",
        "# We'll use this as the base for both G_A (H&E to IHC) and G_B (IHC to H&E).\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_bn=True):\n",
        "        super().__init__()\n",
        "        # Use InstanceNorm2d for CycleGAN as it often performs better for style transfer\n",
        "        # If use_bn is True, use InstanceNorm2d, otherwise Identity\n",
        "        norm_layer = nn.InstanceNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            norm_layer, # Changed from BatchNorm2d to InstanceNorm2d\n",
        "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder (Downsampling path) - 6 ConvBlocks\n",
        "        self.initial_down = ConvBlock(in_channels, features, down=True, act=\"leaky\", use_bn=False) # 3 -> 64 (128x128)\n",
        "        self.down1 = ConvBlock(features, features * 2, down=True, act=\"leaky\") # 64 -> 128 (64x64)\n",
        "        self.down2 = ConvBlock(features * 2, features * 4, down=True, act=\"leaky\") # 128 -> 256 (32x32)\n",
        "        self.down3 = ConvBlock(features * 4, features * 8, down=True, act=\"leaky\") # 256 -> 512 (16x16)\n",
        "        self.down4 = ConvBlock(features * 8, features * 8, down=True, act=\"leaky\") # 512 -> 512 (8x8)\n",
        "        self.down5 = ConvBlock(features * 8, features * 8, down=True, act=\"leaky\") # 512 -> 512 (4x4)\n",
        "        self.down6 = ConvBlock(features * 8, features * 8, down=True, act=\"leaky\") # 512 -> 512 (2x2)\n",
        "\n",
        "        # Bottleneck (from 2x2 to 1x1) - This is the 7th downsampling step\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features * 8, features * 8, 4, 2, 1, padding_mode=\"reflect\"), # 512 -> 512 (1x1)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder (Upsampling path) - 7 ConvBlocks\n",
        "        self.up1 = ConvBlock(features * 8, features * 8, down=False, act=\"relu\", use_bn=True) # 512 -> 512 (skip d6, 2x2)\n",
        "        self.up2 = ConvBlock(features * 8 * 2, features * 8, down=False, act=\"relu\", use_bn=True) # 1024 -> 512 (skip d5, 4x4)\n",
        "        self.up3 = ConvBlock(features * 8 * 2, features * 8, down=False, act=\"relu\", use_bn=True) # 1024 -> 512 (skip d4, 8x8)\n",
        "        self.up4 = ConvBlock(features * 8 * 2, features * 8, down=False, act=\"relu\", use_bn=True) # 1024 -> 512 (skip d3, 16x16)\n",
        "        self.up5 = ConvBlock(features * 8 * 2, features * 4, down=False, act=\"relu\", use_bn=True) # 1024 -> 256 (skip d2, 32x32)\n",
        "        self.up6 = ConvBlock(features * 4 * 2, features * 2, down=False, act=\"relu\", use_bn=True) # 512 -> 128 (skip d1, 64x64)\n",
        "        self.up7 = ConvBlock(features * 2 * 2, features, down=False, act=\"relu\", use_bn=True) # 256 -> 64 (skip initial_down, 128x128)\n",
        "\n",
        "        # Final output layer (no BatchNorm, Tanh activation)\n",
        "        self.final_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features * 2, out_channels, 4, 2, 1), # 128 -> 3 (256x256)\n",
        "            nn.Tanh(), # Output normalized to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder outputs (with skip connections)\n",
        "        d1 = self.initial_down(x) # 64, 128x128\n",
        "        d2 = self.down1(d1) # 128, 64x64\n",
        "        d3 = self.down2(d2) # 256, 32x32\n",
        "        d4 = self.down3(d3) # 512, 16x16\n",
        "        d5 = self.down4(d4) # 512, 8x8\n",
        "        d6 = self.down5(d5) # 512, 4x4\n",
        "        d7 = self.down6(d6) # 512, 2x2\n",
        "\n",
        "        bottleneck = self.bottleneck(d7) # 512, 1x1\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        up1 = self.up1(bottleneck) # 512, 2x2\n",
        "        up2 = self.up2(torch.cat([up1, d7], 1)) # 512+512=1024 -> 512, 4x4\n",
        "        up3 = self.up3(torch.cat([up2, d6], 1)) # 512+512=1024 -> 512, 8x8\n",
        "        up4 = self.up4(torch.cat([up3, d5], 1)) # 512+512=1024 -> 512, 16x16\n",
        "        up5 = self.up5(torch.cat([up4, d4], 1)) # 512+512=1024 -> 256, 32x32\n",
        "        up6 = self.up6(torch.cat([up5, d3], 1)) # 256+256=512 -> 128, 64x64\n",
        "        up7 = self.up7(torch.cat([up6, d2], 1)) # 128+128=256 -> 64, 128x128\n",
        "\n",
        "        # Final output layer\n",
        "        return self.final_up(torch.cat([up7, d1], 1)) # 64+64=128 -> 3, 256x256\n",
        "\n",
        "# --- Discriminator Model (PatchGAN with Spectral Normalization) ---\n",
        "# For CycleGAN, we need two discriminators, one for each domain.\n",
        "# Each Discriminator takes a single image (not concatenated input/output).\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        # Apply spectral_norm to the initial convolutional layer\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_channels, features[0], 4, 2, 1, padding_mode=\"reflect\")),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            # Apply spectral_norm to Conv2d layers within ConvBlock\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.utils.spectral_norm(nn.Conv2d(in_channels, feature, 4, 2, 1, bias=False, padding_mode=\"reflect\")),\n",
        "                    nn.InstanceNorm2d(feature), # Keep InstanceNorm\n",
        "                    nn.LeakyReLU(0.2)\n",
        "                )\n",
        "            )\n",
        "            in_channels = feature\n",
        "\n",
        "        # Apply spectral_norm to the final convolutional layer\n",
        "        layers.append(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_channels, 1, 4, 1, 1, padding_mode=\"reflect\")) # Output a single logit for each patch\n",
        "        )\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        return self.model(x)\n",
        "\n",
        "# --- Initialize Weights (Optional but Good Practice for GANs) ---\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "            # Check if weight exists and is not None before accessing .data\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "            # Check if bias exists and is not None before accessing .data\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0)\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
        "            # Check if weight (gamma) exists and is not None\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                nn.init.normal_(m.weight.data, 1.0, 0.02) # Gamma\n",
        "            # Check if bias (beta) exists and is not None\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0) # Beta\n",
        "\n",
        "# --- Perceptual Loss (VGG-based) ---\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        # Load pre-trained VGG19 features, use only up to relu5_1 for feature extraction\n",
        "        vgg = vgg19(pretrained=True).features[:36].eval().to(device) # Using features up to relu5_1 (index 36)\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.L1Loss() # Use L1 loss on feature maps\n",
        "\n",
        "    def forward(self, input_image, target_image):\n",
        "        # Normalize input images to VGG's expected range [0, 1]\n",
        "        # Our GAN output is [-1, 1], so convert to [0, 1]\n",
        "        input_image_norm = (input_image * 0.5 + 0.5).clamp(0, 1)\n",
        "        target_image_norm = (target_image * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "        # Extract features\n",
        "        features_input = self.vgg(input_image_norm)\n",
        "        features_target = self.vgg(target_image_norm)\n",
        "\n",
        "        # Calculate L1 loss between feature maps\n",
        "        loss = self.criterion(features_input, features_target)\n",
        "        return loss\n",
        "\n",
        "print(\"Models defined for CycleGAN.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4EfVZfGzcKB"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Training Loop for CycleGAN\n",
        "\n",
        "# --- Fake Image Replay Buffer ---\n",
        "# This class stores a history of generated images to prevent discriminator overfitting\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer or trying to create a black hole\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, images):\n",
        "        to_return = []\n",
        "        # Iterate directly over the tensor and detach to prevent graph issues/memory leaks\n",
        "        for image in images.detach():\n",
        "            image = torch.unsqueeze(image, 0) # Add batch dimension back for single image\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(image)\n",
        "                to_return.append(image)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = image\n",
        "                else:\n",
        "                    to_return.append(image)\n",
        "        return torch.cat(to_return)\n",
        "\n",
        "# --- WGAN-GP Loss Functions ---\n",
        "def discriminator_loss_wgan_gp(disc_real_pred, disc_fake_pred, real_img, fake_img, discriminator, lambda_gp, device):\n",
        "    # WGAN-GP: Maximize D(real) - D(fake) - lambda * GP\n",
        "    # Gradient Penalty\n",
        "    alpha = torch.rand(real_img.size(0), 1, 1, 1, device=device)\n",
        "    interpolates = (alpha * real_img + ((1 - alpha) * fake_img)).requires_grad_(True)\n",
        "\n",
        "    disc_interpolates = discriminator(interpolates)\n",
        "\n",
        "    gradients = autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(disc_interpolates, device=device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "\n",
        "    # Discriminator loss: D(fake) - D(real) + gradient_penalty\n",
        "    loss = disc_fake_pred.mean() - disc_real_pred.mean() + gradient_penalty\n",
        "    return loss\n",
        "\n",
        "def generator_loss_wgan_gp(disc_fake_pred):\n",
        "    # WGAN-GP: Minimize -D(fake)\n",
        "    return -disc_fake_pred.mean()\n",
        "\n",
        "\n",
        "# Instantiate models\n",
        "# G_A: H&E (A) -> IHC (B)\n",
        "# G_B: IHC (B) -> H&E (A)\n",
        "gen_A = Generator(in_channels=3, out_channels=3, features=64).to(DEVICE)\n",
        "gen_B = Generator(in_channels=3, out_channels=3, features=64).to(DEVICE)\n",
        "\n",
        "# D_A: Discriminates real H&E from fake H&E\n",
        "# D_B: Discriminates real IHC from fake IHC\n",
        "disc_A = Discriminator(in_channels=3).to(DEVICE)\n",
        "disc_B = Discriminator(in_channels=3).to(DEVICE)\n",
        "\n",
        "# Initialize weights\n",
        "initialize_weights(gen_A)\n",
        "initialize_weights(gen_B)\n",
        "initialize_weights(disc_A)\n",
        "initialize_weights(disc_B)\n",
        "\n",
        "# Loss functions\n",
        "# criterion_GAN is now handled by custom WGAN-GP functions\n",
        "criterion_Cycle = nn.L1Loss() # L1 loss for cycle consistency\n",
        "criterion_Identity = nn.L1Loss() # L1 loss for identity mapping\n",
        "criterion_Perceptual = PerceptualLoss(DEVICE) # New: Perceptual Loss\n",
        "\n",
        "# Optimizers\n",
        "optimizer_gen = optim.Adam(\n",
        "    list(gen_A.parameters()) + list(gen_B.parameters()),\n",
        "    lr=LEARNING_RATE_GEN,\n",
        "    betas=(0.5, 0.999),\n",
        ")\n",
        "optimizer_disc_A = optim.Adam(\n",
        "    disc_A.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999)\n",
        ")\n",
        "optimizer_disc_B = optim.Adam(\n",
        "    disc_B.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999)\n",
        ")\n",
        "\n",
        "# Learning Rate Schedulers (Linear Decay)\n",
        "def lambda_rule(epoch):\n",
        "    # Linear decay from DECAY_EPOCH_START to NUM_EPOCHS\n",
        "    return 1.0 - max(0, epoch - DECAY_EPOCH_START) / float(NUM_EPOCHS - DECAY_EPOCH_START)\n",
        "\n",
        "scheduler_gen = torch.optim.lr_scheduler.LambdaLR(optimizer_gen, lr_lambda=lambda_rule)\n",
        "scheduler_disc_A = torch.optim.lr_scheduler.LambdaLR(optimizer_disc_A, lr_lambda=lambda_rule)\n",
        "scheduler_disc_B = torch.optim.lr_scheduler.LambdaLR(optimizer_disc_B, lr_lambda=lambda_rule)\n",
        "\n",
        "\n",
        "# Initialize start_epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# Initialize GradScaler for AMP\n",
        "scaler = GradScaler(init_scale=2.**10) # Increased initial_scale for more stability with very low LRs\n",
        "\n",
        "# Initialize Replay Buffers\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "# Load checkpoint if resuming training\n",
        "if LOAD_MODEL:\n",
        "    try:\n",
        "        # Load the consolidated checkpoint\n",
        "        checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, \"checkpoint_latest.pth\"), map_location=DEVICE)\n",
        "\n",
        "        gen_A.load_state_dict(checkpoint['gen_A_state_dict'])\n",
        "        gen_B.load_state_dict(checkpoint['gen_B_state_dict'])\n",
        "        disc_A.load_state_dict(checkpoint['disc_A_state_dict'])\n",
        "        disc_B.load_state_dict(checkpoint['disc_B_state_dict'])\n",
        "\n",
        "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
        "        optimizer_disc_A.load_state_dict(checkpoint['optimizer_disc_A_state_dict'])\n",
        "        optimizer_disc_B.load_state_dict(checkpoint['optimizer_disc_B_state_dict'])\n",
        "\n",
        "        start_epoch = checkpoint['epoch'] # Get the epoch to resume from\n",
        "        print(f\"Loaded models and optimizers. Resuming training from epoch {start_epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No latest checkpoint found. Starting training from scratch.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\")\n",
        "        print(\"Starting training from scratch.\") # Fallback to scratch if loading fails\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nStarting CycleGAN training...\")\n",
        "# Loop from start_epoch to NUM_EPOCHS\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    gen_A.train()\n",
        "    gen_B.train()\n",
        "    disc_A.train()\n",
        "    disc_B.train()\n",
        "\n",
        "    # Adjust tqdm description to show actual epoch number\n",
        "    loop = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    avg_gen_loss_total = 0.0\n",
        "    avg_disc_A_loss = 0.0\n",
        "    avg_disc_B_loss = 0.0\n",
        "    avg_perceptual_loss_A = 0.0 # New: Perceptual loss for A->B\n",
        "    avg_perceptual_loss_B = 0.0 # New: Perceptual loss for B->A\n",
        "    avg_gen_adv_A = 0.0 # New: Average G_adv_A loss for logging\n",
        "    avg_gen_adv_B = 0.0 # New: Average G_adv_B loss for logging\n",
        "    avg_cycle_A = 0.0   # New: Average Cycle_A loss for logging\n",
        "    avg_cycle_B = 0.0   # New: Average Cycle_B loss for logging\n",
        "    avg_identity_A = 0.0 # New: Average Identity_A loss for logging\n",
        "    avg_identity_B = 0.0 # New: Average Identity_B loss for logging\n",
        "\n",
        "\n",
        "    for idx, (real_A, real_B) in enumerate(loop):\n",
        "        real_A = real_A.to(DEVICE) # H&E\n",
        "        real_B = real_B.to(DEVICE) # IHC\n",
        "\n",
        "        # --- Train Discriminator D_B (for IHC) ---\n",
        "        optimizer_disc_B.zero_grad()\n",
        "        with autocast(DEVICE): # Use updated autocast syntax\n",
        "            # Generate fake IHC from real H&E\n",
        "            fake_B = gen_A(real_A)\n",
        "\n",
        "            # Discriminate real IHC\n",
        "            disc_real_B_pred = disc_B(real_B)\n",
        "\n",
        "            # Discriminate fake IHC from replay buffer\n",
        "            fake_B_from_buffer = fake_B_buffer.push_and_pop(fake_B)\n",
        "            disc_fake_B_pred = disc_B(fake_B_from_buffer.detach())\n",
        "\n",
        "            # WGAN-GP loss components (without GP term yet)\n",
        "            loss_disc_B_adv = disc_fake_B_pred.mean() - disc_real_B_pred.mean()\n",
        "\n",
        "        # Backward pass for adv loss, then unscale for GP calculation in FP32\n",
        "        scaler.scale(loss_disc_B_adv).backward(retain_graph=True) # retain_graph for GP\n",
        "        scaler.unscale_(optimizer_disc_B) # Unscale gradients for this optimizer\n",
        "\n",
        "        # Calculate D_B Gradient Penalty in FP32\n",
        "        # The GP calculation itself should be outside autocast if it was causing issues,\n",
        "        # but the current setup with unscale_() should handle it.\n",
        "        loss_disc_B_gp = discriminator_loss_wgan_gp(\n",
        "            disc_real_B_pred, disc_fake_B_pred, real_B, fake_B_from_buffer, disc_B, LAMBDA_GP, DEVICE\n",
        "        ) # Removed .item() here, let it be a tensor for proper backward if needed\n",
        "\n",
        "        # Backward pass for GP\n",
        "        scaler.scale(loss_disc_B_gp).backward() # Backward the GP term\n",
        "\n",
        "        # Total D_B loss for logging (sum of adv and gp)\n",
        "        loss_disc_B_total = loss_disc_B_adv.item() + loss_disc_B_gp.item() # Use .item() for logging\n",
        "\n",
        "        # Clip gradients before optimizer step\n",
        "        torch.nn.utils.clip_grad_norm_(disc_B.parameters(), max_norm=5.0)\n",
        "\n",
        "        scaler.step(optimizer_disc_B)\n",
        "        scaler.update()\n",
        "\n",
        "        # --- Train Discriminator D_A (for H&E) ---\n",
        "        optimizer_disc_A.zero_grad()\n",
        "        with autocast(DEVICE): # Use updated autocast syntax\n",
        "            # Generate fake H&E from real IHC\n",
        "            fake_A = gen_B(real_B)\n",
        "\n",
        "            # Discriminate real H&E\n",
        "            disc_real_A_pred = disc_A(real_A)\n",
        "\n",
        "            # Discriminate fake H&E from replay buffer\n",
        "            fake_A_from_buffer = fake_A_buffer.push_and_pop(fake_A)\n",
        "            disc_fake_A_pred = disc_A(fake_A_from_buffer.detach())\n",
        "\n",
        "            # WGAN-GP loss components (without GP term yet)\n",
        "            loss_disc_A_adv = disc_fake_A_pred.mean() - disc_real_A_pred.mean()\n",
        "\n",
        "        # Backward pass for adv loss, then unscale for GP calculation in FP32\n",
        "        scaler.scale(loss_disc_A_adv).backward(retain_graph=True) # retain_graph for GP\n",
        "        scaler.unscale_(optimizer_disc_A) # Unscale gradients for this optimizer\n",
        "\n",
        "        # Calculate D_A Gradient Penalty in FP32\n",
        "        loss_disc_A_gp = discriminator_loss_wgan_gp(\n",
        "            disc_real_A_pred, disc_fake_A_pred, real_A, fake_A_from_buffer, disc_A, LAMBDA_GP, DEVICE\n",
        "        ) # Removed .item() here\n",
        "\n",
        "        # Backward pass for GP\n",
        "        scaler.scale(loss_disc_A_gp).backward() # Backward the GP term\n",
        "\n",
        "        # Total D_A loss for logging\n",
        "        loss_disc_A_total = loss_disc_A_adv.item() + loss_disc_A_gp.item()\n",
        "\n",
        "        # Clip gradients before optimizer step\n",
        "        torch.nn.utils.clip_grad_norm_(disc_A.parameters(), max_norm=5.0)\n",
        "\n",
        "        scaler.step(optimizer_disc_A)\n",
        "        scaler.update()\n",
        "\n",
        "        # --- Train Generators G_A and G_B ---\n",
        "        optimizer_gen.zero_grad()\n",
        "\n",
        "        # Combine all generator losses into a single autocast block and single backward pass\n",
        "        with autocast(DEVICE): # Use updated autocast syntax\n",
        "            # Generate fake images\n",
        "            fake_B = gen_A(real_A)\n",
        "            fake_A = gen_B(real_B)\n",
        "\n",
        "            # Adversarial Loss (Generators try to fool Discriminators)\n",
        "            loss_gen_adv_A_val = generator_loss_wgan_gp(disc_B(fake_B)) # G_A tries to fool D_B\n",
        "            loss_gen_adv_B_val = generator_loss_wgan_gp(disc_A(fake_A)) # G_B tries to fool D_A\n",
        "\n",
        "            # Cycle Consistency Loss (A -> B -> A and B -> A -> B)\n",
        "            cycled_A = gen_B(fake_B) # real_A -> fake_B -> cycled_A\n",
        "            loss_cycle_A_val = criterion_Cycle(cycled_A, real_A)\n",
        "\n",
        "            cycled_B = gen_A(fake_A) # real_B -> fake_A -> cycled_B\n",
        "            loss_cycle_B_val = criterion_Cycle(cycled_B, real_B)\n",
        "\n",
        "            # Identity Loss (Optional, but helps preserve color/content)\n",
        "            identity_B = gen_A(real_B)\n",
        "            loss_identity_B_val = criterion_Identity(identity_B, real_B)\n",
        "\n",
        "            identity_A = gen_B(real_A)\n",
        "            loss_identity_A_val = criterion_Identity(identity_A, real_A)\n",
        "\n",
        "            # Perceptual Loss (ensure inputs are float for VGG, even within autocast)\n",
        "            # VGG expects FP32, so explicitly cast if fake_B/fake_A are FP16 from autocast\n",
        "            loss_perceptual_A_val = criterion_Perceptual(fake_B.float(), real_B)\n",
        "            loss_perceptual_B_val = criterion_Perceptual(fake_A.float(), real_A)\n",
        "\n",
        "            # Total Generator Loss\n",
        "            loss_gen_total = (\n",
        "                loss_gen_adv_A_val\n",
        "                + loss_gen_adv_B_val\n",
        "                + LAMBDA_CYCLE * loss_cycle_A_val\n",
        "                + LAMBDA_CYCLE * loss_cycle_B_val\n",
        "                + LAMBDA_IDENTITY * loss_identity_A_val\n",
        "                + LAMBDA_IDENTITY * loss_identity_B_val\n",
        "                + LAMBDA_PERCEPTUAL * loss_perceptual_A_val\n",
        "                + LAMBDA_PERCEPTUAL * loss_perceptual_B_val\n",
        "            )\n",
        "\n",
        "        # Single backward pass for all generator losses\n",
        "        scaler.scale(loss_gen_total).backward()\n",
        "\n",
        "        # Clip gradients before optimizer step\n",
        "        torch.nn.utils.clip_grad_norm_(list(gen_A.parameters()) + list(gen_B.parameters()), max_norm=5.0)\n",
        "\n",
        "        scaler.step(optimizer_gen)\n",
        "        scaler.update()\n",
        "\n",
        "        # --- Update Averages and Progress Bar ---\n",
        "        avg_disc_A_loss += loss_disc_A_total # Use total D loss for logging\n",
        "        avg_disc_B_loss += loss_disc_B_total # Use total D loss for logging\n",
        "        avg_gen_loss_total += loss_gen_total.item() # Use .item() for logging\n",
        "        avg_gen_adv_A += loss_gen_adv_A_val.item() # Accumulate for average\n",
        "        avg_gen_adv_B += loss_gen_adv_B_val.item() # Accumulate for average\n",
        "        avg_cycle_A += loss_cycle_A_val.item()     # Accumulate for average\n",
        "        avg_cycle_B += loss_cycle_B_val.item()     # Accumulate for average\n",
        "        avg_identity_A += loss_identity_A_val.item() # Accumulate for average\n",
        "        avg_identity_B += loss_identity_B_val.item() # Accumulate for average\n",
        "        avg_perceptual_loss_A += loss_perceptual_A_val.item()\n",
        "        avg_perceptual_loss_B += loss_perceptual_B_val.item()\n",
        "\n",
        "\n",
        "        loop.set_postfix(\n",
        "            D_A_loss=loss_disc_A_total, # Log total D loss\n",
        "            D_B_loss=loss_disc_B_total, # Log total D loss\n",
        "            G_total_loss=loss_gen_total.item(),\n",
        "            G_adv_A=loss_gen_adv_A_val.item(), # Log last batch value for postfix\n",
        "            G_adv_B=loss_gen_adv_B_val.item(), # Log last batch value for postfix\n",
        "            Cycle_A=loss_cycle_A_val.item(),\n",
        "            Cycle_B=loss_cycle_B_val.item(),\n",
        "            Identity_A=loss_identity_A_val.item(),\n",
        "            Identity_B=loss_identity_B_val.item(),\n",
        "            Percept_A=loss_perceptual_A_val.item(),\n",
        "            Percept_B=loss_perceptual_B_val.item(),\n",
        "        )\n",
        "\n",
        "    # --- End of Epoch ---\n",
        "    avg_disc_A_loss /= len(train_loader)\n",
        "    avg_disc_B_loss /= len(train_loader)\n",
        "    avg_gen_loss_total /= len(train_loader)\n",
        "    avg_gen_adv_A /= len(train_loader) # Calculate epoch average\n",
        "    avg_gen_adv_B /= len(train_loader) # Calculate epoch average\n",
        "    avg_cycle_A /= len(train_loader)   # Calculate epoch average\n",
        "    avg_cycle_B /= len(train_loader)   # Calculate epoch average\n",
        "    avg_identity_A /= len(train_loader) # Calculate epoch average\n",
        "    avg_identity_B /= len(train_loader) # Calculate epoch average\n",
        "    avg_perceptual_loss_A /= len(train_loader)\n",
        "    avg_perceptual_loss_B /= len(train_loader)\n",
        "\n",
        "    # Step learning rate schedulers\n",
        "    scheduler_gen.step()\n",
        "    scheduler_disc_A.step()\n",
        "    scheduler_disc_B.step()\n",
        "\n",
        "    # Append loss history to CSV\n",
        "    with open(TRAINING_LOG_PATH, 'a') as f:\n",
        "        # Only write header if starting a new log or if file is empty\n",
        "        if epoch == start_epoch or (epoch == 0 and os.path.getsize(TRAINING_LOG_PATH) == 0):\n",
        "            f.write(\"Epoch,Avg_D_A_Loss,Avg_D_B_Loss,Avg_G_Total_Loss,Avg_G_Adv_A,Avg_G_Adv_B,Avg_Cycle_A,Avg_Cycle_B,Avg_Identity_A,Avg_Identity_B,Avg_Percept_A,Avg_Percept_B\\n\") # Updated header\n",
        "        f.write(f\"{epoch+1},{avg_disc_A_loss:.6f},{avg_disc_B_loss:.6f},{avg_gen_loss_total:.6f},{avg_gen_adv_A:.6f},{avg_gen_adv_B:.6f},{avg_cycle_A:.6f},{avg_cycle_B:.6f},{avg_identity_A:.6f},{avg_identity_B:.6f},{avg_perceptual_loss_A:.6f},{avg_perceptual_loss_B:.6f}\\n\") # Updated data with averages\n",
        "\n",
        "    # Print formatted table every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0 or epoch == start_epoch: # Print header and first epoch of the run\n",
        "        if epoch == start_epoch: # Print header only on the first epoch of this run\n",
        "            print(\"\\n\" + \"=\"*160) # Extended width for new columns\n",
        "            print(f\"{'Epoch':<8} | {'D_A Loss':<10} | {'D_B Loss':<10} | {'G_Total Loss':<14} | {'G_Adv_A':<10} | {'G_Adv_B':<10} | {'Cycle_A':<10} | {'Cycle_B':<10} | {'Identity_A':<12} | {'Identity_B':<12} | {'Percept_A':<11} | {'Percept_B':<11}\") # Updated header\n",
        "            print(\"=\"*160) # Extended width\n",
        "        print(f\"{epoch+1:<8} | {avg_disc_A_loss:<10.4f} | {avg_disc_B_loss:<10.4f} | {avg_gen_loss_total:<14.4f} | {avg_gen_adv_A:<10.4f} | {avg_gen_adv_B:<10.4f} | {avg_cycle_A:<10.4f} | {avg_cycle_B:<10.4f} | {avg_identity_A:<12.4f} | {avg_identity_B:<12.4f} | {avg_perceptual_loss_A:<11.4f} | {avg_perceptual_loss_B:<11.4f}\") # Updated data with averages\n",
        "\n",
        "\n",
        "    # Save checkpoint\n",
        "    if SAVE_MODEL and (epoch + 1) % 10 == 0:\n",
        "        # Consolidate all states into one dictionary\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'gen_A_state_dict': gen_A.state_dict(),\n",
        "            'gen_B_state_dict': gen_B.state_dict(),\n",
        "            'disc_A_state_dict': disc_A.state_dict(),\n",
        "            'disc_B_state_dict': disc_B.state_dict(),\n",
        "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
        "            'optimizer_disc_A_state_dict': optimizer_disc_A.state_dict(),\n",
        "            'optimizer_disc_B_state_dict': optimizer_disc_B.state_dict(),\n",
        "            'scheduler_gen_state_dict': scheduler_gen.state_dict(), # Save scheduler state\n",
        "            'scheduler_disc_A_state_dict': scheduler_disc_A.state_dict(), # Save scheduler state\n",
        "            'scheduler_disc_B_state_dict': scheduler_disc_B.state_dict(), # Save scheduler state\n",
        "            'scaler_state_dict': scaler.state_dict(), # Save scaler state\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, \"checkpoint_latest.pth\"))\n",
        "        print(f\"Models, optimizers, schedulers, and scaler saved at epoch {epoch+1}\")\n",
        "\n",
        "    # Save sample images (using validation data)\n",
        "    gen_A.eval() # G_A: H&E -> IHC\n",
        "    gen_B.eval() # G_B: IHC -> H&E\n",
        "    with torch.no_grad():\n",
        "        for i, (val_A, val_B) in enumerate(val_loader):\n",
        "            if i >= 1: break # Save only one sample per epoch for brevity\n",
        "            val_A = val_A.to(DEVICE)\n",
        "            val_B = val_B.to(DEVICE)\n",
        "\n",
        "            # Use autocast for inference as well for consistency, though not strictly needed for speed\n",
        "            with autocast(DEVICE): # Use updated autocast syntax\n",
        "                # Generate fake IHC from H&E\n",
        "                fake_B_val = gen_A(val_A)\n",
        "                # Cycle back to H&E\n",
        "                cycled_A_val = gen_B(fake_B_val)\n",
        "\n",
        "                # Generate fake H&E from IHC\n",
        "                fake_A_val = gen_B(val_B)\n",
        "                # Cycle back to IHC\n",
        "                cycled_B_val = gen_A(fake_A_val)\n",
        "\n",
        "            # Denormalize for saving\n",
        "            val_A_denorm = (val_A * 0.5 + 0.5).clamp(0, 1)\n",
        "            val_B_denorm = (val_B * 0.5 + 0.5).clamp(0, 1)\n",
        "            fake_B_val_denorm = (fake_B_val * 0.5 + 0.5).clamp(0, 1)\n",
        "            cycled_A_val_denorm = (cycled_A_val * 0.5 + 0.5).clamp(0, 1)\n",
        "            fake_A_val_denorm = (fake_A_val * 0.5 + 0.5).clamp(0, 1)\n",
        "            cycled_B_val_denorm = (cycled_B_val * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "            # Save generated and cycled images\n",
        "            save_image(val_A_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"real_A_he_sample.png\"))\n",
        "            save_image(val_B_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"real_B_ihc_sample.png\"))\n",
        "            save_image(fake_B_val_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"generated_B_ihc_epoch_{epoch+1}.png\"))\n",
        "            save_image(cycled_A_val_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"cycled_A_he_epoch_{epoch+1}.png\"))\n",
        "            save_image(fake_A_val_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"generated_A_he_epoch_{epoch+1}.png\"))\n",
        "            save_image(cycled_B_val_denorm, os.path.join(SAMPLE_IMAGES_DIR, f\"cycled_B_ihc_epoch_{epoch+1}.png\"))\n",
        "\n",
        "    gen_A.train()\n",
        "    gen_B.train() # Set back to train mode\n",
        "\n",
        "print(\"\\nCycleGAN training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H1: metrics libs + imports =====\n",
        "!pip -q install torch-fidelity==0.3.0 lpips==0.1.4 piq==0.8.0\n",
        "\n",
        "import os, json, random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch_fidelity import calculate_metrics  # FID/KID\n",
        "import lpips                                   # LPIPS\n",
        "import piq                                     # SSIM/PSNR\n",
        "\n",
        "print(\"Ready on:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "09ww0LlGY7KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H3: Eval config & pairing =====\n",
        "# Use the same val split as vanilla\n",
        "VAL_DIR_A = '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "VAL_DIR_B = '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "EVAL_TAG = \"hybrid_epoch_200\"\n",
        "OUT_ROOT = \"/content/drive/MyDrive/HER2/hybrid_eval\"\n",
        "GEN_DIR  = Path(OUT_ROOT) / f\"eval_{EVAL_TAG}_A2B\"      # generated A->B\n",
        "LOG_DIR  = Path(OUT_ROOT) / f\"metrics_{EVAL_TAG}\"       # metrics\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 256  # keep consistent with training\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n",
        "    return sorted([p for p in Path(folder).rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "paths_A = list_images(VAL_DIR_A)\n",
        "paths_B = list_images(VAL_DIR_B)\n",
        "bname_to_B = {Path(p).stem: p for p in paths_B}\n",
        "pairs = [(str(pA), bname_to_B[Path(pA).stem]) for pA in paths_A if Path(pA).stem in bname_to_B]\n",
        "\n",
        "print(f\"A images: {len(paths_A)} | B images: {len(paths_B)} | Paired matches: {len(pairs)}\")\n",
        "\n",
        "# Transforms\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,)*3, (0.5,)*3),\n",
        "])\n",
        "to_pil = T.Compose([T.Lambda(lambda x: (x * 0.5 + 0.5).clamp(0,1)), T.ToPILImage()])\n",
        "\n",
        "# For LPIPS/SSIM/PSNR (both sides same size)\n",
        "resize_01 = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
        "    T.ToTensor(),  # [0,1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "RxeS0KX7ZNNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H4: Generate A->B for all valA =====\n",
        "with torch.no_grad():\n",
        "    for pA in paths_A:\n",
        "        out_path = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "        if out_path.exists():\n",
        "            continue\n",
        "        imgA = Image.open(pA).convert('RGB')\n",
        "        x = to_tensor(imgA).unsqueeze(0).to(DEVICE)\n",
        "        y = gen_A_h(x)[0].cpu()            # [-1,1]\n",
        "        to_pil(y).save(out_path)\n",
        "\n",
        "print(f\"Saved {len(list(GEN_DIR.glob('*.png')))} generated images to {GEN_DIR}\")\n"
      ],
      "metadata": {
        "id": "2y1ttrC3dAHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H5: FID & KID =====\n",
        "fidkid = calculate_metrics(\n",
        "    input1=str(GEN_DIR),\n",
        "    input2=str(VAL_DIR_B),\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, fid=True, kid=True, prc=False, verbose=False\n",
        ")\n",
        "with open(LOG_DIR / \"fid_kid.json\", \"w\") as f:\n",
        "    json.dump(fidkid, f, indent=2)\n",
        "print(json.dumps(fidkid, indent=2))\n"
      ],
      "metadata": {
        "id": "0mQXl4r8dGSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H6: LPIPS / SSIM / PSNR on paired matches =====\n",
        "lpips_fn = lpips.LPIPS(net='vgg').to(DEVICE).eval()\n",
        "\n",
        "rows = []\n",
        "with torch.no_grad():\n",
        "    for pA, pB in pairs:\n",
        "        gen_path = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "        if not gen_path.exists():\n",
        "            continue\n",
        "\n",
        "        # Resize both sides to the same eval size (256)\n",
        "        G01 = resize_01(Image.open(gen_path).convert('RGB')).to(DEVICE).unsqueeze(0)\n",
        "        B01 = resize_01(Image.open(pB).convert('RGB')).to(DEVICE).unsqueeze(0)\n",
        "\n",
        "        # LPIPS expects [-1,1]\n",
        "        Gm1p1, Bm1p1 = G01*2-1, B01*2-1\n",
        "        lp = lpips_fn(Gm1p1, Bm1p1).item()\n",
        "\n",
        "        ssim = piq.ssim(G01, B01, data_range=1.0).item()\n",
        "        psnr = piq.psnr(G01, B01, data_range=1.0).item()\n",
        "\n",
        "        rows.append({\"basename\": Path(pA).stem, \"lpips\": lp, \"ssim\": ssim, \"psnr\": psnr})\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"basename\")\n",
        "df.to_csv(LOG_DIR / \"paired_metrics.csv\", index=False)\n",
        "\n",
        "summary = {\n",
        "    \"tag\": EVAL_TAG,\n",
        "    \"N_pairs\": int(len(df)),\n",
        "    \"LPIPS_mean\": float(df.lpips.mean()),\n",
        "    \"LPIPS_std\": float(df.lpips.std(ddof=0)),\n",
        "    \"SSIM_mean\": float(df.ssim.mean()),\n",
        "    \"SSIM_std\": float(df.ssim.std(ddof=0)),\n",
        "    \"PSNR_mean\": float(df.psnr.mean()),\n",
        "    \"PSNR_std\": float(df.psnr.std(ddof=0)),\n",
        "}\n",
        "with open(LOG_DIR / \"paired_metrics_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n"
      ],
      "metadata": {
        "id": "AJt0PBeKdKpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H7: Triplet grid for the thesis =====\n",
        "import matplotlib.pyplot as plt\n",
        "random.seed(42)\n",
        "\n",
        "N = 12  # number of triplets\n",
        "sample_pairs = pairs[:]\n",
        "random.shuffle(sample_pairs)\n",
        "sample_pairs = sample_pairs[:N]\n",
        "\n",
        "ncols, nrows = 3, N\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(9, 3*N), dpi=150)\n",
        "\n",
        "for i, (pA, pB) in enumerate(sample_pairs):\n",
        "    gen = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "    A = Image.open(pA).convert('RGB')\n",
        "    B = Image.open(pB).convert('RGB')\n",
        "    G = Image.open(gen).convert('RGB')\n",
        "\n",
        "    axes[i,0].imshow(A); axes[i,0].set_title(\"H&E (A)\"); axes[i,0].axis('off')\n",
        "    axes[i,1].imshow(B); axes[i,1].set_title(\"IHC – Expected (B)\"); axes[i,1].axis('off')\n",
        "    axes[i,2].imshow(G); axes[i,2].set_title(\"IHC – Generated (A→B)\"); axes[i,2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "FIG_PATH = Path(OUT_ROOT) / f\"eval_{EVAL_TAG}_A-B-G_triplets.png\"\n",
        "plt.savefig(FIG_PATH, bbox_inches='tight')\n",
        "print(\"Saved figure:\", FIG_PATH)\n"
      ],
      "metadata": {
        "id": "WGubD7txdxfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plain Cycle GAN"
      ],
      "metadata": {
        "id": "9AfpNxuVBg37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 1: Imports and Global Configuration =====\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "\n",
        "# (Optional) Pretty loss tables\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "# --- Mount Google Drive (Colab) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Your dataset folders (pix2pix-style)\n",
        "    TRAIN_DIR_A: str = '/content/drive/MyDrive/HER2/TrainValAB/trainA'\n",
        "    TRAIN_DIR_B: str = '/content/drive/MyDrive/HER2/TrainValAB/trainB'\n",
        "    VAL_DIR_A:   str = '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "    VAL_DIR_B:   str = '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "    # Experiment output (new folder for vanilla CycleGAN)\n",
        "    EXP_ROOT: str = '/content/drive/MyDrive/HER2/cyclegan_vanilla'\n",
        "    SAVE_EVERY: int = 10      # checkpoint cadence\n",
        "    SAMPLE_EVERY: int = 10    # visualization cadence\n",
        "\n",
        "    # Training\n",
        "    EPOCHS: int = 200\n",
        "    BATCH_SIZE: int = 1\n",
        "    IMG_SIZE: int = 256\n",
        "    SEED: int = 42\n",
        "    DEVICE: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "    # Optimizer (vanilla CycleGAN uses LSGAN; these are standard)\n",
        "    LR: float = 2e-4\n",
        "    BETA1: float = 0.5\n",
        "    BETA2: float = 0.999\n",
        "    LR_DECAY_START: int = 100  # start linear decay here\n",
        "\n",
        "    # Loss weights\n",
        "    LAMBDA_CYCLE: float = 10.0\n",
        "    LAMBDA_ID: float = 5.0     # 0.5 * LAMBDA_CYCLE\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# Create output dirs\n",
        "exp_dir = Path(cfg.EXP_ROOT)\n",
        "ckpt_dir = exp_dir / 'checkpoints'\n",
        "samples_dir = exp_dir / 'samples'\n",
        "logs_dir = exp_dir / 'logs'\n",
        "for d in [exp_dir, ckpt_dir, samples_dir, logs_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(cfg.SEED)\n",
        "torch.manual_seed(cfg.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(cfg.SEED)\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "print(\"Config:\", asdict(cfg))\n",
        "print(\"Outputs ->\", str(exp_dir))\n",
        "print(\"Device:\", cfg.DEVICE)\n"
      ],
      "metadata": {
        "id": "IHhqk38LJkAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 2: Dataset Class and DataLoader =====\n",
        "class UnpairedAB(Dataset):\n",
        "    \"\"\"\n",
        "    Expects four folders:\n",
        "      trainA/, trainB/, valA/, valB/\n",
        "    Unpaired by default. For visualization, tries to match by filename (if names align).\n",
        "    \"\"\"\n",
        "    def __init__(self, dir_A, dir_B, augment=False):\n",
        "        self.dir_A = Path(dir_A)\n",
        "        self.dir_B = Path(dir_B)\n",
        "        exts = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')\n",
        "        self.paths_A = sorted([p for p in self.dir_A.rglob('*') if p.suffix.lower() in exts])\n",
        "        self.paths_B = sorted([p for p in self.dir_B.rglob('*') if p.suffix.lower() in exts])\n",
        "        if len(self.paths_A) == 0 or len(self.paths_B) == 0:\n",
        "            raise RuntimeError(f\"No images found in:\\n{self.dir_A}\\n{self.dir_B}\")\n",
        "\n",
        "        # Basic aug (optional)\n",
        "        t = [transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC)]\n",
        "        if augment:\n",
        "            t.append(transforms.RandomHorizontalFlip())\n",
        "            t.append(transforms.RandomVerticalFlip())\n",
        "        t += [transforms.ToTensor(), transforms.Normalize((0.5,)*3, (0.5,)*3)]\n",
        "        self.tf = transforms.Compose(t)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Unpaired sampling\n",
        "        return max(len(self.paths_A), len(self.paths_B))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_A = self.paths_A[idx % len(self.paths_A)]\n",
        "        # Try to pick a B with the same basename; else random\n",
        "        base = path_A.stem\n",
        "        same = [p for p in self.paths_B if p.stem == base]\n",
        "        path_B = same[0] if same else self.paths_B[random.randint(0, len(self.paths_B)-1)]\n",
        "\n",
        "        img_A = Image.open(path_A).convert('RGB')\n",
        "        img_B = Image.open(path_B).convert('RGB')\n",
        "        return self.tf(img_A), self.tf(img_B), path_A.name, path_B.name\n",
        "\n",
        "# DataLoaders\n",
        "train_ds = UnpairedAB(cfg.TRAIN_DIR_A, cfg.TRAIN_DIR_B, augment=True)\n",
        "val_ds   = UnpairedAB(cfg.VAL_DIR_A,   cfg.VAL_DIR_B,   augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
        "\n",
        "# Helpers for visualization\n",
        "inv_norm = transforms.Normalize(mean=[-1,-1,-1], std=[2,2,2])\n",
        "def denorm(x):  # [-1,1] -> [0,1]\n",
        "    return (x * 0.5 + 0.5).clamp(0,1)\n"
      ],
      "metadata": {
        "id": "90f9Q7E2Nf8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 3: Model Definitions (Generators & Discriminators) =====\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3, padding=0),\n",
        "            nn.InstanceNorm2d(dim, affine=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3, padding=0),\n",
        "            nn.InstanceNorm2d(dim, affine=False),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_filters=64, n_blocks=9):\n",
        "        super().__init__()\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_c, n_filters, 7, padding=0),\n",
        "            nn.InstanceNorm2d(n_filters, affine=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        curr = n_filters\n",
        "        # Down 2x\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(curr, curr*2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(curr*2, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            curr *= 2\n",
        "        # Res blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResnetBlock(curr)]\n",
        "        # Up 2x\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(curr, curr//2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(curr//2, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            curr //= 2\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(curr, out_c, 7, padding=0),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_c=3, n_filters=64):\n",
        "        super().__init__()\n",
        "        def block(in_f, out_f, norm=True):\n",
        "            layers = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n",
        "            if norm: layers.insert(1, nn.InstanceNorm2d(out_f, affine=False))\n",
        "            return layers\n",
        "        self.model = nn.Sequential(\n",
        "            *block(in_c, n_filters, norm=False),\n",
        "            *block(n_filters, n_filters*2),\n",
        "            *block(n_filters*2, n_filters*4),\n",
        "            nn.Conv2d(n_filters*4, 1, 4, stride=1, padding=1),  # Patch score\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Instantiate models\n",
        "G_A2B = ResnetGenerator().to(cfg.DEVICE)\n",
        "G_B2A = ResnetGenerator().to(cfg.DEVICE)\n",
        "D_A   = PatchDiscriminator().to(cfg.DEVICE)\n",
        "D_B   = PatchDiscriminator().to(cfg.DEVICE)\n",
        "\n",
        "# Optimizers\n",
        "opt_G   = torch.optim.Adam(list(G_A2B.parameters()) + list(G_B2A.parameters()), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "opt_D_A = torch.optim.Adam(D_A.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "opt_D_B = torch.optim.Adam(D_B.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "\n",
        "# Schedulers: linear decay after LR_DECAY_START\n",
        "def lambda_rule(epoch):\n",
        "    if epoch < cfg.LR_DECAY_START:\n",
        "        return 1.0\n",
        "    return 1.0 - (epoch - cfg.LR_DECAY_START) / float(max(1, cfg.EPOCHS - cfg.LR_DECAY_START))\n",
        "\n",
        "sch_G   = torch.optim.lr_scheduler.LambdaLR(opt_G,   lr_lambda=lambda_rule)\n",
        "sch_D_A = torch.optim.lr_scheduler.LambdaLR(opt_D_A, lr_lambda=lambda_rule)\n",
        "sch_D_B = torch.optim.lr_scheduler.LambdaLR(opt_D_B, lr_lambda=lambda_rule)\n",
        "\n",
        "# Losses (LSGAN + L1)\n",
        "adv_criterion   = nn.MSELoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "\n",
        "# Image pools (stabilize D)\n",
        "class ImagePool:\n",
        "    def __init__(self, pool_size=50):\n",
        "        self.pool_size = pool_size\n",
        "        self.images = []\n",
        "    def query(self, images):\n",
        "        if self.pool_size == 0:\n",
        "            return images\n",
        "        out = []\n",
        "        for img in images:\n",
        "            img = img.detach()\n",
        "            if len(self.images) < self.pool_size:\n",
        "                self.images.append(img)\n",
        "                out.append(img)\n",
        "            else:\n",
        "                if random.random() > 0.5:\n",
        "                    idx = random.randint(0, self.pool_size - 1)\n",
        "                    tmp = self.images[idx].clone()\n",
        "                    self.images[idx] = img\n",
        "                    out.append(tmp)\n",
        "                else:\n",
        "                    out.append(img)\n",
        "        return torch.stack(out, dim=0)\n",
        "\n",
        "pool_A = ImagePool(50)\n",
        "pool_B = ImagePool(50)\n",
        "\n",
        "# Resume if latest exists\n",
        "start_epoch = 1\n",
        "latest = ckpt_dir / 'latest.pt'\n",
        "if latest.exists():\n",
        "    print(\"Resuming from\", latest)\n",
        "    s = torch.load(latest, map_location=cfg.DEVICE)\n",
        "    G_A2B.load_state_dict(s['G_A2B']); G_B2A.load_state_dict(s['G_B2A'])\n",
        "    D_A.load_state_dict(s['D_A']);     D_B.load_state_dict(s['D_B'])\n",
        "    opt_G.load_state_dict(s['opt_G']); opt_D_A.load_state_dict(s['opt_D_A']); opt_D_B.load_state_dict(s['opt_D_B'])\n",
        "    sch_G.load_state_dict(s['sch_G']); sch_D_A.load_state_dict(s['sch_D_A']); sch_D_B.load_state_dict(s['sch_D_B'])\n",
        "    start_epoch = s['epoch'] + 1\n",
        "    print(\"Start epoch:\", start_epoch)\n"
      ],
      "metadata": {
        "id": "HUH6gYvyNxJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 4 (REPLACED): Training Loop for CycleGAN with tqdm progress bar =====\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "    obj = {\n",
        "        'epoch': epoch,\n",
        "        'G_A2B': G_A2B.state_dict(),\n",
        "        'G_B2A': G_B2A.state_dict(),\n",
        "        'D_A': D_A.state_dict(),\n",
        "        'D_B': D_B.state_dict(),\n",
        "        'opt_G': opt_G.state_dict(),\n",
        "        'opt_D_A': opt_D_A.state_dict(),\n",
        "        'opt_D_B': opt_D_B.state_dict(),\n",
        "        'sch_G': sch_G.state_dict(),\n",
        "        'sch_D_A': sch_D_A.state_dict(),\n",
        "        'sch_D_B': sch_D_B.state_dict(),\n",
        "        'cfg': asdict(cfg),\n",
        "    }\n",
        "    path = ckpt_dir / f'epoch_{epoch:03d}.pt'\n",
        "    torch.save(obj, path)\n",
        "    torch.save(obj, ckpt_dir / 'latest.pt')\n",
        "    print(f'[Checkpoint] Saved {path}')\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_epoch(epoch):\n",
        "    G_A2B.eval(); G_B2A.eval()\n",
        "    try:\n",
        "        img_A, img_B, name_A, name_B = next(iter(val_loader))\n",
        "    except StopIteration:\n",
        "        return\n",
        "    img_A = img_A.to(cfg.DEVICE); img_B = img_B.to(cfg.DEVICE)\n",
        "    fake_B = G_A2B(img_A)\n",
        "\n",
        "    paired = (name_A[0].split('.')[0] == name_B[0].split('.')[0])\n",
        "    title_B = \"Expected (paired B)\" if paired else \"Reference (real B, unpaired)\"\n",
        "\n",
        "    grid = make_grid(torch.cat([denorm(img_A), denorm(img_B), denorm(fake_B)], dim=0), nrow=img_A.size(0))\n",
        "    out_path = samples_dir / f'epoch_{epoch:03d}.png'\n",
        "    save_image(grid, out_path)\n",
        "    print(f\"[Sample] Saved {out_path}\")\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.imshow(grid.permute(1,2,0).cpu().numpy()); plt.axis('off')\n",
        "    plt.title(f\"Epoch {epoch} — Left: Actual (A), Middle: {title_B}, Right: Generated (A→B)\")\n",
        "    plt.show()\n",
        "\n",
        "class LossBook:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "    def add(self, epoch, **kw):\n",
        "        row = {'epoch': epoch}; row.update({k: float(v) for k,v in kw.items()})\n",
        "        self.rows.append(row)\n",
        "    def table(self):\n",
        "        if pd is None: return None\n",
        "        return pd.DataFrame(self.rows)\n",
        "\n",
        "lossbook = LossBook()\n",
        "\n",
        "real_label = 1.0\n",
        "fake_label = 0.0\n",
        "\n",
        "for epoch in range(start_epoch, cfg.EPOCHS + 1):\n",
        "    G_A2B.train(); G_B2A.train(); D_A.train(); D_B.train()\n",
        "\n",
        "    sums = {k:0.0 for k in [\n",
        "        'G_total','G_adv_A2B','G_adv_B2A','cycle_A','cycle_B','id_A','id_B','D_A','D_B'\n",
        "    ]}\n",
        "    nb = 0\n",
        "\n",
        "    # tqdm progress bar over train batches\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg.EPOCHS}\", ncols=100, leave=False)\n",
        "    for imgs_A, imgs_B, _, _ in pbar:\n",
        "        imgs_A = imgs_A.to(cfg.DEVICE)\n",
        "        imgs_B = imgs_B.to(cfg.DEVICE)\n",
        "\n",
        "        # === Generators ===\n",
        "        opt_G.zero_grad()\n",
        "\n",
        "        # Identity: G_A2B(B) ≈ B, G_B2A(A) ≈ A\n",
        "        id_B = G_A2B(imgs_B)\n",
        "        loss_id_B = recon_criterion(id_B, imgs_B) * cfg.LAMBDA_ID\n",
        "        id_A = G_B2A(imgs_A)\n",
        "        loss_id_A = recon_criterion(id_A, imgs_A) * cfg.LAMBDA_ID\n",
        "\n",
        "        # GAN: A->B\n",
        "        fake_B = G_A2B(imgs_A)\n",
        "        pred_fake_B = D_B(fake_B)\n",
        "        valid_B = torch.ones_like(pred_fake_B, device=cfg.DEVICE)\n",
        "        loss_G_A2B = adv_criterion(pred_fake_B, valid_B)\n",
        "\n",
        "        # GAN: B->A\n",
        "        fake_A = G_B2A(imgs_B)\n",
        "        pred_fake_A = D_A(fake_A)\n",
        "        valid_A = torch.ones_like(pred_fake_A, device=cfg.DEVICE)\n",
        "        loss_G_B2A = adv_criterion(pred_fake_A, valid_A)\n",
        "\n",
        "        # Cycle\n",
        "        rec_A = G_B2A(fake_B)\n",
        "        rec_B = G_A2B(fake_A)\n",
        "        loss_cyc_A = recon_criterion(rec_A, imgs_A) * cfg.LAMBDA_CYCLE\n",
        "        loss_cyc_B = recon_criterion(rec_B, imgs_B) * cfg.LAMBDA_CYCLE\n",
        "\n",
        "        loss_G = loss_G_A2B + loss_G_B2A + loss_cyc_A + loss_cyc_B + loss_id_A + loss_id_B\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # === D_A ===\n",
        "        opt_D_A.zero_grad()\n",
        "        pred_real_A = D_A(imgs_A)\n",
        "        valid = torch.ones_like(pred_real_A, device=cfg.DEVICE)\n",
        "        loss_D_A_real = adv_criterion(pred_real_A, valid)\n",
        "\n",
        "        fake_A_pool = pool_A.query(fake_A)\n",
        "        pred_fake_A = D_A(fake_A_pool.detach())\n",
        "        fake = torch.zeros_like(pred_fake_A, device=cfg.DEVICE)\n",
        "        loss_D_A_fake = adv_criterion(pred_fake_A, fake)\n",
        "        loss_DA = 0.5*(loss_D_A_real + loss_D_A_fake)\n",
        "        loss_DA.backward()\n",
        "        opt_D_A.step()\n",
        "\n",
        "        # === D_B ===\n",
        "        opt_D_B.zero_grad()\n",
        "        pred_real_B = D_B(imgs_B)\n",
        "        valid = torch.ones_like(pred_real_B, device=cfg.DEVICE)\n",
        "        loss_D_B_real = adv_criterion(pred_real_B, valid)\n",
        "\n",
        "        fake_B_pool = pool_B.query(fake_B)\n",
        "        pred_fake_B = D_B(fake_B_pool.detach())\n",
        "        fake = torch.zeros_like(pred_fake_B, device=cfg.DEVICE)\n",
        "        loss_D_B_fake = adv_criterion(pred_fake_B, fake)\n",
        "        loss_DB = 0.5*(loss_D_B_real + loss_D_B_fake)\n",
        "        loss_DB.backward()\n",
        "        opt_D_B.step()\n",
        "\n",
        "        # Accumulate + update progress bar postfix\n",
        "        sums['G_total']    += loss_G.item()\n",
        "        sums['G_adv_A2B']  += loss_G_A2B.item()\n",
        "        sums['G_adv_B2A']  += loss_G_B2A.item()\n",
        "        sums['cycle_A']    += loss_cyc_A.item()\n",
        "        sums['cycle_B']    += loss_cyc_B.item()\n",
        "        sums['id_A']       += loss_id_A.item()\n",
        "        sums['id_B']       += loss_id_B.item()\n",
        "        sums['D_A']        += loss_DA.item()\n",
        "        sums['D_B']        += loss_DB.item()\n",
        "        nb += 1\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"G\": f\"{sums['G_total']/nb:.3f}\",\n",
        "            \"D_A\": f\"{sums['D_A']/nb:.3f}\",\n",
        "            \"D_B\": f\"{sums['D_B']/nb:.3f}\",\n",
        "        })\n",
        "\n",
        "    # Step schedulers once per epoch\n",
        "    sch_G.step(); sch_D_A.step(); sch_D_B.step()\n",
        "\n",
        "    means = {k: v/max(1,nb) for k,v in sums.items()}\n",
        "    lossbook.add(epoch, **means)\n",
        "\n",
        "    # End-of-epoch summary line\n",
        "    print(f\"Epoch {epoch:03d}/{cfg.EPOCHS} | \"\n",
        "          f\"G:{means['G_total']:.4f} | D_A:{means['D_A']:.4f} D_B:{means['D_B']:.4f} | \"\n",
        "          f\"cycA:{means['cycle_A']:.3f} cycB:{means['cycle_B']:.3f} | idA:{means['id_A']:.3f} idB:{means['id_B']:.3f}\")\n",
        "\n",
        "    # Sample + table every SAMPLE_EVERY\n",
        "    if epoch % cfg.SAMPLE_EVERY == 0:\n",
        "        visualize_epoch(epoch)\n",
        "        if pd is not None:\n",
        "            df = lossbook.table()\n",
        "            print(df.to_string(index=False))\n",
        "        else:\n",
        "            print(\"[Info] Install pandas for a formatted loss table.\")\n",
        "\n",
        "    # Checkpoint every SAVE_EVERY (and on final epoch)\n",
        "    if epoch % cfg.SAVE_EVERY == 0 or epoch == cfg.EPOCHS:\n",
        "        save_checkpoint(epoch)\n",
        "\n",
        "# Save final CSV and show final sample\n",
        "if pd is not None:\n",
        "    df = lossbook.table()\n",
        "    csv_path = logs_dir / 'epoch_losses.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"[Log] Saved loss CSV to {csv_path}\")\n",
        "\n",
        "visualize_epoch(epoch=cfg.EPOCHS)\n"
      ],
      "metadata": {
        "id": "mY0uyXUXN92D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5: Install metrics libs (run once per fresh Colab) =====\n",
        "!pip -q install torch-fidelity==0.3.0 lpips==0.1.4 piq==0.8.0\n"
      ],
      "metadata": {
        "id": "2poIe2kOOBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_fidelity import calculate_metrics"
      ],
      "metadata": {
        "id": "gwDRH8SJHcgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 6: Eval config, pairing, helpers =====\n",
        "import os, glob, random, json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch_fidelity import calculate_metrics  # torch-fidelity\n",
        "import lpips  # perceptual\n",
        "import piq    # SSIM/PSNR\n",
        "\n",
        "# --- Choose the split you want to evaluate (val or test) ---\n",
        "EVAL_DIR_A = cfg.VAL_DIR_A  # '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "EVAL_DIR_B = cfg.VAL_DIR_B  # '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "# --- Where to dump generated images & results for this checkpoint ---\n",
        "EVAL_TAG       = \"epoch_150\"  # <-- change to the checkpoint you want to evaluate\n",
        "GEN_OUT_DIR    = samples_dir / f\"eval_{EVAL_TAG}_A2B\"\n",
        "METRICS_OUTDIR = logs_dir / f\"metrics_{EVAL_TAG}\"\n",
        "GEN_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "METRICS_OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Transforms to feed the net, and to convert back to PIL for saving\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,)*3, (0.5,)*3)\n",
        "])\n",
        "to_pil = T.Compose([T.Lambda(lambda x: (x * 0.5 + 0.5).clamp(0,1)), T.ToPILImage()])\n",
        "\n",
        "# Pair files by basename (only for LPIPS/SSIM/PSNR)\n",
        "def list_images(folder):\n",
        "    exts = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n",
        "    return sorted([p for p in Path(folder).rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "paths_A = list_images(EVAL_DIR_A)\n",
        "paths_B = list_images(EVAL_DIR_B)\n",
        "bname_to_B = {Path(p).stem: p for p in paths_B}\n",
        "pairs = []\n",
        "for pA in paths_A:\n",
        "    bn = Path(pA).stem\n",
        "    if bn in bname_to_B:\n",
        "        pairs.append((str(pA), bname_to_B[bn]))\n",
        "\n",
        "print(f\"Found {len(paths_A)} A images, {len(paths_B)} B images, and {len(pairs)} paired matches by filename.\")\n"
      ],
      "metadata": {
        "id": "6KrYspntHGzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 7: Load checkpoint & export A→B translations =====\n",
        "# Load the checkpoint you want to evaluate\n",
        "ckpt = torch.load(ckpt_dir / f\"{EVAL_TAG}.pt\", map_location=cfg.DEVICE)\n",
        "G_A2B.load_state_dict(ckpt['G_A2B'])\n",
        "G_B2A.load_state_dict(ckpt['G_B2A'])\n",
        "G_A2B.eval(); G_B2A.eval()\n",
        "\n",
        "# Generate A->B for all A images in EVAL_DIR_A\n",
        "with torch.no_grad():\n",
        "    for pA in paths_A:\n",
        "        imgA = Image.open(pA).convert('RGB')\n",
        "        x = to_tensor(imgA).unsqueeze(0).to(cfg.DEVICE)\n",
        "        y_fake = G_A2B(x)[0].cpu()\n",
        "        pil = to_pil(y_fake)\n",
        "        out_path = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "        pil.save(out_path)\n",
        "\n",
        "print(f\"Saved {len(paths_A)} generated images to {GEN_OUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "fFreP7U0HLHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 8: Compute FID & KID =====\n",
        "# torch-fidelity expects directories with images\n",
        "metrics = calculate_metrics(\n",
        "    input1=str(GEN_OUT_DIR),\n",
        "    input2=str(EVAL_DIR_B),\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, fid=True, kid=True, prc=False, verbose=False\n",
        ")\n",
        "with open(METRICS_OUTDIR / \"fid_kid.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(json.dumps(metrics, indent=2))\n"
      ],
      "metadata": {
        "id": "MsseFNoAH8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 9 (REPLACED): LPIPS, SSIM, PSNR on paired matches =====\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import lpips\n",
        "import piq\n",
        "\n",
        "# set eval size = training size used by the nets\n",
        "EVAL_SIZE = cfg.IMG_SIZE\n",
        "\n",
        "# transforms\n",
        "to_unit = T.ToTensor()  # [0,1]\n",
        "resize_01 = T.Compose([\n",
        "    T.Resize((EVAL_SIZE, EVAL_SIZE), interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
        "    T.ToTensor(),  # [0,1]\n",
        "])\n",
        "def to_m1p1(x01):  # [0,1] -> [-1,1]\n",
        "    return x01 * 2 - 1\n",
        "\n",
        "lpips_fn = lpips.LPIPS(net='vgg').to(cfg.DEVICE).eval()\n",
        "\n",
        "rows = []\n",
        "with torch.no_grad():\n",
        "    for pA, pB in pairs:\n",
        "        gen_path = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "        if not gen_path.exists():\n",
        "            continue\n",
        "\n",
        "        # Load & resize both to the SAME size\n",
        "        Gimg01 = resize_01(Image.open(gen_path).convert('RGB')).to(cfg.DEVICE).unsqueeze(0)  # [1,3,H,W], 0..1\n",
        "        Bimg01 = resize_01(Image.open(pB).convert('RGB')).to(cfg.DEVICE).unsqueeze(0)\n",
        "\n",
        "        # LPIPS expects [-1,1]\n",
        "        Gm1p1 = to_m1p1(Gimg01)\n",
        "        Bm1p1 = to_m1p1(Bimg01)\n",
        "        lp = lpips_fn(Gm1p1, Bm1p1).item()\n",
        "\n",
        "        # SSIM / PSNR on [0,1]\n",
        "        ssim_val = piq.ssim(Gimg01, Bimg01, data_range=1.0).item()\n",
        "        psnr_val = piq.psnr(Gimg01, Bimg01, data_range=1.0).item()\n",
        "\n",
        "        rows.append({\n",
        "            \"basename\": Path(pA).stem,\n",
        "            \"lpips\": lp,\n",
        "            \"ssim\": ssim_val,\n",
        "            \"psnr\": psnr_val\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"basename\")\n",
        "df.to_csv(METRICS_OUTDIR / \"paired_metrics.csv\", index=False)\n",
        "\n",
        "agg = {\n",
        "    \"N_pairs\": int(len(df)),\n",
        "    \"LPIPS_mean\": float(df[\"lpips\"].mean()),\n",
        "    \"LPIPS_std\": float(df[\"lpips\"].std(ddof=0)),\n",
        "    \"SSIM_mean\": float(df[\"ssim\"].mean()),\n",
        "    \"SSIM_std\": float(df[\"ssim\"].std(ddof=0)),\n",
        "    \"PSNR_mean\": float(df[\"psnr\"].mean()),\n",
        "    \"PSNR_std\": float(df[\"psnr\"].std(ddof=0)),\n",
        "}\n",
        "with open(METRICS_OUTDIR / \"paired_metrics_summary.json\", \"w\") as f:\n",
        "    json.dump(agg, f, indent=2)\n",
        "\n",
        "print(agg)\n"
      ],
      "metadata": {
        "id": "yIu5nqrcItq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 10: Create a grid of examples for the thesis =====\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "N = 12  # how many triplets to show\n",
        "sample_pairs = pairs[:]\n",
        "random.shuffle(sample_pairs)\n",
        "sample_pairs = sample_pairs[:N]\n",
        "\n",
        "ncols = 3\n",
        "nrows = N\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(9, 3*N), dpi=150)\n",
        "\n",
        "for i, (pA, pB) in enumerate(sample_pairs):\n",
        "    gen = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "    A = Image.open(pA).convert('RGB')\n",
        "    B = Image.open(pB).convert('RGB')\n",
        "    G = Image.open(gen).convert('RGB')\n",
        "\n",
        "    axes[i,0].imshow(A); axes[i,0].set_title(\"H&E (A)\"); axes[i,0].axis('off')\n",
        "    axes[i,1].imshow(B); axes[i,1].set_title(\"IHC – Expected (B)\"); axes[i,1].axis('off')\n",
        "    axes[i,2].imshow(G); axes[i,2].set_title(\"IHC – Generated (A→B)\"); axes[i,2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig_path = samples_dir / f\"eval_{EVAL_TAG}_A-B-G_triplets.png\"\n",
        "plt.savefig(fig_path, bbox_inches='tight')\n",
        "print(f\"Saved figure: {fig_path}\")\n"
      ],
      "metadata": {
        "id": "aCmD3HCQJE-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "_V7J6tENfcCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {\n",
        "    # 1) Pix2Pix (paired)\n",
        "    \"pix2pix\": {\n",
        "        \"type\": \"pix2pix\",\n",
        "        \"ckpt\": \"/content/drive/MyDrive/HER2/pix2pix_checkpoints/generator_epoch_200.pth\",\n",
        "        \"out\":  \"/content/drive/MyDrive/HER2/eval_all_models/pix2pix\"\n",
        "    },\n",
        "\n",
        "    # 2) CycleGAN-Base (ResNet-9) — saved under EXP_ROOT/checkpoints/\n",
        "    #    If epoch_200.pt is missing, use latest.pt instead.\n",
        "    \"cyclegan_base\": {\n",
        "        \"type\": \"cyclegan_resnet9\",\n",
        "        \"ckpt\": \"/content/drive/MyDrive/HER2/cyclegan_vanilla/checkpoints/epoch_200.pt\",  # fallback: .../latest.pt\n",
        "        \"out\":  \"/content/drive/MyDrive/HER2/eval_all_models/cyclegan_base\"\n",
        "    },\n",
        "\n",
        "    # 3) CycleGAN-UNet (your “after pix” section)\n",
        "    \"cyclegan_unet\": {\n",
        "        \"type\": \"cyclegan_unet\",\n",
        "        \"ckpt\": \"/content/drive/MyDrive/HER2/cyclegan_checkpoints/checkpoint_epoch_200.pth\",\n",
        "        \"out\":  \"/content/drive/MyDrive/HER2/eval_all_models/cyclegan_unet\"\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "J1kYwxvyKRgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(model_type: str):\n",
        "    if model_type == \"pix2pix\":\n",
        "        # From your notebook: class Pix2PixUNet(nn.Module)\n",
        "        G = Pix2PixUNet().to(DEVICE)\n",
        "\n",
        "    elif model_type == \"cyclegan_resnet9\":\n",
        "        # From your notebook: class ResnetGenerator(nn.Module)\n",
        "        # Signature: ResnetGenerator(input_nc, output_nc, ngf=64, n_blocks=9)\n",
        "        G = ResnetGenerator(3, 3, n_blocks=9).to(DEVICE)\n",
        "\n",
        "    elif model_type == \"cyclegan_unet\":\n",
        "        # From your notebook: class CycleUNet(nn.Module)\n",
        "        G = CycleUNet().to(DEVICE)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "    G.eval()\n",
        "    return G\n"
      ],
      "metadata": {
        "id": "EtTUTIu5f9xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "VAL_DIR_A = Path(\"/content/drive/MyDrive/HER2/TrainValAB/valA\")  # H&E\n",
        "VAL_DIR_B = Path(\"/content/drive/MyDrive/HER2/TrainValAB/valB\")  # IHC\n"
      ],
      "metadata": {
        "id": "qRPSYURygJDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-locate valA / valB in Google Drive and set VAL_DIR_A / VAL_DIR_B\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Ensure Drive is mounted:\n",
        "# from google.colab import drive; drive.mount('/content/drive')\n",
        "\n",
        "ROOTS = [\n",
        "    Path(\"/content/drive/MyDrive/HER2/TrainValAB\"),\n",
        "    Path(\"/content/drive/MyDrive/HER2\"),\n",
        "    Path(\"/content/drive/MyDrive\"),\n",
        "]\n",
        "\n",
        "def is_img_dir(p: Path):\n",
        "    exts = {\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\",\".webp\"}\n",
        "    return p.is_dir() and any((p/f).suffix.lower() in exts for f in os.listdir(p) if (p/f).is_file())\n",
        "\n",
        "found_A, found_B = None, None\n",
        "\n",
        "def ci_eq(a,b): return a.lower()==b.lower()\n",
        "\n",
        "# search by walking the roots\n",
        "for root in ROOTS:\n",
        "    if not root.exists(): continue\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        # prefer shallow matches first\n",
        "        for d in dirnames:\n",
        "            p = Path(dirpath)/d\n",
        "            if ci_eq(d, \"valA\") and is_img_dir(p) and found_A is None:\n",
        "                found_A = p\n",
        "            if ci_eq(d, \"valB\") and is_img_dir(p) and found_B is None:\n",
        "                found_B = p\n",
        "        if found_A and found_B:\n",
        "            break\n",
        "    if found_A and found_B:\n",
        "        break\n",
        "\n",
        "# fallback: use trainA/trainB if valA/valB absent\n",
        "if found_A is None or found_B is None:\n",
        "    for root in ROOTS:\n",
        "        if not root.exists(): continue\n",
        "        candA = next((Path(dirpath)/d for dirpath, dirnames, _ in os.walk(root)\n",
        "                      for d in dirnames if ci_eq(d,\"trainA\")), None)\n",
        "        candB = next((Path(dirpath)/d for dirpath, dirnames, _ in os.walk(root)\n",
        "                      for d in dirnames if ci_eq(d,\"trainB\")), None)\n",
        "        if candA and candB and is_img_dir(candA) and is_img_dir(candB):\n",
        "            print(\"[WARN] valA/valB not found; falling back to trainA/trainB.\")\n",
        "            found_A, found_B = candA, candB\n",
        "            break\n",
        "\n",
        "assert found_A and found_B, \"Couldn't find valA/valB (or trainA/trainB) anywhere under MyDrive.\"\n",
        "\n",
        "VAL_DIR_A, VAL_DIR_B = found_A, found_B\n",
        "print(\"VAL_DIR_A:\", VAL_DIR_A)\n",
        "print(\"VAL_DIR_B:\", VAL_DIR_B)\n",
        "print(\"A count:\", len([f for f in os.listdir(VAL_DIR_A) if (VAL_DIR_A/f).is_file()]))\n",
        "print(\"B count:\", len([f for f in os.listdir(VAL_DIR_B) if (VAL_DIR_B/f).is_file()]))\n"
      ],
      "metadata": {
        "id": "F5FKSknfg1yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLTK5PkoHX3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNWdMbDDdgrpawwa4rZYR88",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}