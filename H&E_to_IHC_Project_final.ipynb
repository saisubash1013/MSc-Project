{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisubash1013/MSc-Project/blob/main/H%26E_to_IHC_Project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dkw9Xg8C6nl"
      },
      "source": [
        "# Pix2Pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6iIqmV7-TkN"
      },
      "outputs": [],
      "source": [
        "# Pix2Pix — imports & basic setup\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# pick GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# dataset root in Drive (expects TrainValAB/{trainA,trainB,valA,valB})\n",
        "DATA_PATH = \"/content/drive/MyDrive/HER2/TrainValAB\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxfSoKabC_C7"
      },
      "outputs": [],
      "source": [
        "# Cell 2 — mount Drive and point to dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# expected folders: TrainValAB/{trainA, trainB, valA, valB}\n",
        "TRAIN_A_DIR = os.path.join(DATA_PATH, \"trainA\")\n",
        "TRAIN_B_DIR = os.path.join(DATA_PATH, \"trainB\")\n",
        "VALA_DIR    = os.path.join(DATA_PATH, \"valA\")\n",
        "VALB_DIR    = os.path.join(DATA_PATH, \"valB\")\n",
        "CKPT_DIR    = \"/content/drive/MyDrive/checkpointsPix2Pix\"\n",
        "\n",
        "# quick sanity checks\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset root not found: {DATA_PATH}\"\n",
        "for d in [TRAIN_A_DIR, TRAIN_B_DIR, VALA_DIR, VALB_DIR]:\n",
        "    print(f\"{os.path.basename(d):7s}:\", \"OK\" if os.path.exists(d) else \"MISSING\")\n",
        "\n",
        "# count images (jpg/png/jpeg)\n",
        "def count_imgs(p):\n",
        "    if not os.path.exists(p): return 0\n",
        "    exts = (\".jpg\", \".jpeg\", \".png\")\n",
        "    return sum(f.lower().endswith(exts) for f in os.listdir(p))\n",
        "\n",
        "print(f\"trainA: {count_imgs(TRAIN_A_DIR)}  |  trainB: {count_imgs(TRAIN_B_DIR)}\")\n",
        "print(f\"valA  : {count_imgs(VALA_DIR)}  |  valB  : {count_imgs(VALB_DIR)}\")\n",
        "\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503o7hVBEHFb"
      },
      "outputs": [],
      "source": [
        "# Cell 3 — U-Net generator (Pix2Pix)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # downsampling (H/2,W/2 each step)\n",
        "        self.enc1 = self._down(3,   64, norm=False)\n",
        "        self.enc2 = self._down(64,  128)\n",
        "        self.enc3 = self._down(128, 256)\n",
        "        self.enc4 = self._down(256, 512)\n",
        "        self.enc5 = self._down(512, 512)\n",
        "\n",
        "        # upsampling + skip connections\n",
        "        self.dec1 = self._up(512,   512)\n",
        "        self.dec2 = self._up(1024,  256)\n",
        "        self.dec3 = self._up(512,   128)\n",
        "        self.dec4 = self._up(256,    64)\n",
        "        self.dec5 = nn.ConvTranspose2d(128, 3, 4, 2, 1)  # back to 3 channels\n",
        "\n",
        "    @staticmethod\n",
        "    def _down(i, o, norm=True):\n",
        "        layers = [nn.Conv2d(i, o, 4, 2, 1)]\n",
        "        if norm: layers.append(nn.BatchNorm2d(o))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def _up(i, o):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(i, o, 4, 2, 1),\n",
        "            nn.BatchNorm2d(o),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        e5 = self.enc5(e4)\n",
        "\n",
        "        d1 = self.dec1(e5); d1 = torch.cat([d1, e4], dim=1)\n",
        "        d2 = self.dec2(d1); d2 = torch.cat([d2, e3], dim=1)\n",
        "        d3 = self.dec3(d2); d3 = torch.cat([d3, e2], dim=1)\n",
        "        d4 = self.dec4(d3); d4 = torch.cat([d4, e1], dim=1)\n",
        "\n",
        "        return torch.tanh(self.dec5(d4))  # outputs in [-1, 1]\n",
        "\n",
        "generator = Generator().to(device)\n",
        "print(\"Generator ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--LQJCyESrV"
      },
      "outputs": [],
      "source": [
        "# Cell 4 — PatchGAN discriminator (Pix2Pix)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 4, 2, 1),  nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 1, 1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 1),  nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_in, x_tgt):\n",
        "        # condition on (input, target) pair\n",
        "        x = torch.cat([x_in, x_tgt], dim=1)\n",
        "        return self.net(x)  # patch-wise real/fake map\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "print(\"Discriminator ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly3W4QSJEeSX"
      },
      "outputs": [],
      "source": [
        "# Cell 5 — training setup (losses, optimizers, params)\n",
        "criterion_GAN = nn.BCELoss()   # real/fake\n",
        "criterion_L1  = nn.L1Loss()    # pixel L1\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "num_epochs  = 10\n",
        "lambda_L1   = 100\n",
        "start_epoch = 0\n",
        "\n",
        "print(\"Training setup ready → epochs:\", num_epochs, \"| λ_L1:\", lambda_L1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSUijSjiEzDC"
      },
      "outputs": [],
      "source": [
        "# Cell 6 — resume training from a saved checkpoint\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    generator.load_state_dict(ckpt['generator_state_dict'])\n",
        "    discriminator.load_state_dict(ckpt['discriminator_state_dict'])\n",
        "    optimizer_G.load_state_dict(ckpt['optimizer_G_state_dict'])\n",
        "    optimizer_D.load_state_dict(ckpt['optimizer_D_state_dict'])\n",
        "    epoch = ckpt.get('epoch', 0)\n",
        "    print(f\"Resumed from epoch {epoch}\")\n",
        "    return epoch\n",
        "\n",
        "# To resume, uncomment and point to a file:\n",
        "# start_epoch = load_checkpoint('/content/drive/MyDrive/checkpointsPix2Pix/checkpoint_epoch_5.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJh4vxtFJnI"
      },
      "outputs": [],
      "source": [
        "# Cell 6.5 — dataset + dataloader (creates `train_loader`)\n",
        "class PairedFolderDataset(Dataset):\n",
        "    def __init__(self, root, sub_a='trainA', sub_b='trainB', size=256):\n",
        "        self.dir_a = os.path.join(root, sub_a)\n",
        "        self.dir_b = os.path.join(root, sub_b)\n",
        "\n",
        "        exts = ('.jpg', '.png', '.jpeg')\n",
        "        self.files_a = sorted([f for f in os.listdir(self.dir_a) if f.lower().endswith(exts)])\n",
        "        self.files_b = sorted([f for f in os.listdir(self.dir_b) if f.lower().endswith(exts)])\n",
        "\n",
        "        # pair by index (assumes aligned ordering/filenames)\n",
        "        self.length = min(len(self.files_a), len(self.files_b))\n",
        "\n",
        "        # to 256×256, tensors, normalize to [-1, 1] (matches generator tanh)\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.Resize((size, size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_a = Image.open(os.path.join(self.dir_a, self.files_a[idx])).convert('RGB')\n",
        "        img_b = Image.open(os.path.join(self.dir_b, self.files_b[idx])).convert('RGB')\n",
        "        return self.tf(img_a), self.tf(img_b)\n",
        "\n",
        "# build loader\n",
        "train_dataset = PairedFolderDataset(DATA_PATH, 'trainA', 'trainB', size=256)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train pairs: {len(train_dataset)}  |  Batch size: {train_loader.batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — training loop + per-epoch checkpoints\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "training_losses = {'g': [], 'd': []}\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    g_sum = 0.0\n",
        "    d_sum = 0.0\n",
        "\n",
        "    for i, (he_images, ihc_images) in enumerate(train_loader):\n",
        "        he_images  = he_images.to(device)\n",
        "        ihc_images = ihc_images.to(device)\n",
        "\n",
        "        # PatchGAN targets (30×30 map)\n",
        "        b = he_images.size(0)\n",
        "        real_labels = torch.ones(b, 1, 30, 30, device=device)\n",
        "        fake_labels = torch.zeros(b, 1, 30, 30, device=device)\n",
        "\n",
        "        # --- Discriminator ---\n",
        "        optimizer_D.zero_grad()\n",
        "        out_real   = discriminator(he_images, ihc_images)\n",
        "        d_real     = criterion_GAN(out_real, real_labels)\n",
        "\n",
        "        fake_ihc   = generator(he_images)\n",
        "        out_fake   = discriminator(he_images, fake_ihc.detach())\n",
        "        d_fake     = criterion_GAN(out_fake, fake_labels)\n",
        "\n",
        "        d_loss = 0.5 * (d_real + d_fake)\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # --- Generator ---\n",
        "        optimizer_G.zero_grad()\n",
        "        out_fake = discriminator(he_images, fake_ihc)\n",
        "        g_gan    = criterion_GAN(out_fake, real_labels)\n",
        "        g_l1     = criterion_L1(fake_ihc, ihc_images)\n",
        "        g_loss   = g_gan + lambda_L1 * g_l1\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_sum += g_loss.item()\n",
        "        d_sum += d_loss.item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Step {i:04d}/{len(train_loader)} | D {d_loss.item():.4f} | G {g_loss.item():.4f}\")\n",
        "\n",
        "    # epoch averages\n",
        "    g_avg = g_sum / max(1, len(train_loader))\n",
        "    d_avg = d_sum / max(1, len(train_loader))\n",
        "    training_losses['g'].append(g_avg)\n",
        "    training_losses['d'].append(d_avg)\n",
        "\n",
        "    # save checkpoint\n",
        "    ckpt = {\n",
        "        'epoch': epoch + 1,\n",
        "        'generator_state_dict': generator.state_dict(),\n",
        "        'discriminator_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "        'g_loss': g_avg,\n",
        "        'd_loss': d_avg,\n",
        "        'training_losses': training_losses\n",
        "    }\n",
        "    torch.save(ckpt, os.path.join(CKPT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | G {g_avg:.4f} | D {d_avg:.4f} — saved\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "ODDRqXuhqj4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — quick validation preview (save a few H&E | Real IHC | Fake IHC triptychs)\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# small helper: [-1,1] → [0,1]\n",
        "def denorm(x):\n",
        "    return (x * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "# build a val loader (same pairing logic)\n",
        "val_dataset = PairedFolderDataset(DATA_PATH, 'valA', 'valB', size=256)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "SAMPLE_DIR = os.path.join(CKPT_DIR, \"samples_val\")\n",
        "os.makedirs(SAMPLE_DIR, exist_ok=True)\n",
        "\n",
        "# generate N samples and save triptychs\n",
        "N = min(12, len(val_dataset))\n",
        "was_training = generator.training\n",
        "generator.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (he, ihc) in enumerate(val_loader):\n",
        "        if i >= N: break\n",
        "        he  = he.to(device)\n",
        "        ihc = ihc.to(device)\n",
        "\n",
        "        fake = generator(he)\n",
        "        trip = torch.cat([denorm(he), denorm(ihc), denorm(fake)], dim=3)  # concat width-wise\n",
        "        save_image(trip.cpu(), os.path.join(SAMPLE_DIR, f\"val_{i:03d}_triptych.png\"))\n",
        "\n",
        "if was_training:\n",
        "    generator.train()\n",
        "\n",
        "print(f\"Saved {N} triptychs to: {SAMPLE_DIR}\")\n"
      ],
      "metadata": {
        "id": "Z4Pjv6txZubH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Generate 10 triptychs from the given epoch-200 checkpoint ===\n",
        "import os, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/HER2/pix2pix_checkpoints/generator_epoch_200.pth\"\n",
        "VALA_DIR = \"/content/drive/MyDrive/HER2/TrainValAB/valA\"  # H&E\n",
        "VALB_DIR = \"/content/drive/MyDrive/HER2/TrainValAB/valB\"  # IHC\n",
        "OUT_DIR  = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "assert os.path.exists(CHECKPOINT_PATH), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
        "assert os.path.exists(VALA_DIR) and os.path.exists(VALB_DIR), \"valA/valB paths not found.\"\n",
        "\n",
        "# ---- Model defs ----\n",
        "class Pix2PixUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1=self._c(3,64,False); self.enc2=self._c(64,128); self.enc3=self._c(128,256)\n",
        "        self.enc4=self._c(256,512); self.enc5=self._c(512,512)\n",
        "        self.dec1=self._u(512,512); self.dec2=self._u(1024,256)\n",
        "        self.dec3=self._u(512,128); self.dec4=self._u(256,64)\n",
        "        self.dec5=nn.ConvTranspose2d(128,3,4,2,1)\n",
        "    def _c(self,i,o,norm=True):\n",
        "        layers=[nn.Conv2d(i,o,4,2,1)]\n",
        "        if norm: layers.append(nn.BatchNorm2d(o))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        return nn.Sequential(*layers)\n",
        "    def _u(self,i,o):\n",
        "        return nn.Sequential(nn.ConvTranspose2d(i,o,4,2,1), nn.BatchNorm2d(o), nn.ReLU())\n",
        "    def forward(self,x):\n",
        "        e1=self.enc1(x); e2=self.enc2(e1); e3=self.enc3(e2); e4=self.enc4(e3); e5=self.enc5(e4)\n",
        "        d1=self.dec1(e5); d1=torch.cat([d1,e4],1)\n",
        "        d2=self.dec2(d1); d2=torch.cat([d2,e3],1)\n",
        "        d3=self.dec3(d2); d3=torch.cat([d3,e2],1)\n",
        "        d4=self.dec4(d3); d4=torch.cat([d4,e1],1)\n",
        "        return torch.tanh(self.dec5(d4))\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, down=True, act=\"relu\", use_bn=True):\n",
        "        super().__init__()\n",
        "        norm = nn.InstanceNorm2d(out_ch) if use_bn else nn.Identity()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False, padding_mode=\"reflect\") if down\n",
        "            else nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),\n",
        "            norm, nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    def forward(self,x): return self.conv(x)\n",
        "\n",
        "class CycleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, feat=64):\n",
        "        super().__init__()\n",
        "        self.initial_down=ConvBlock(in_ch,feat,True,\"leaky\",False)\n",
        "        self.down1=ConvBlock(feat,feat*2,True,\"leaky\"); self.down2=ConvBlock(feat*2,feat*4,True,\"leaky\")\n",
        "        self.down3=ConvBlock(feat*4,feat*8,True,\"leaky\"); self.down4=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.down5=ConvBlock(feat*8,feat*8,True,\"leaky\"); self.down6=ConvBlock(feat*8,feat*8,True,\"leaky\")\n",
        "        self.bottleneck=nn.Sequential(nn.Conv2d(feat*8,feat*8,4,2,1,padding_mode=\"reflect\"), nn.ReLU())\n",
        "        self.up1=ConvBlock(feat*8,feat*8,False,\"relu\",True)\n",
        "        self.up2=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up3=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up4=ConvBlock(feat*16,feat*8,False,\"relu\",True)\n",
        "        self.up5=ConvBlock(feat*16,feat*4,False,\"relu\",True)\n",
        "        self.up6=ConvBlock(feat*8,feat*2,False,\"relu\",True)\n",
        "        self.up7=ConvBlock(feat*4,feat,False,\"relu\",True)\n",
        "        self.final_up=nn.Sequential(nn.ConvTranspose2d(feat*2,out_ch,4,2,1), nn.Tanh())\n",
        "    def forward(self,x):\n",
        "        d1=self.initial_down(x); d2=self.down1(d1); d3=self.down2(d2)\n",
        "        d4=self.down3(d3); d5=self.down4(d4); d6=self.down5(d5); d7=self.down6(d6)\n",
        "        b=self.bottleneck(d7)\n",
        "        u1=self.up1(b); u2=self.up2(torch.cat([u1,d7],1))\n",
        "        u3=self.up3(torch.cat([u2,d6],1)); u4=self.up4(torch.cat([u3,d5],1))\n",
        "        u5=self.up5(torch.cat([u4,d4],1)); u6=self.up6(torch.cat([u5,d3],1))\n",
        "        u7=self.up7(torch.cat([u6,d2],1))\n",
        "        return self.final_up(torch.cat([u7,d1],1))\n",
        "\n",
        "# ---- load checkpoint and detect arch ----\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "sd = None\n",
        "if isinstance(ckpt, dict):\n",
        "    for k in [\"generator_state_dict\",\"gen_A_state_dict\",\"state_dict\",\"model\",\"netG\",\"G\"]:\n",
        "        if k in ckpt and isinstance(ckpt[k], dict):\n",
        "            sd = ckpt[k]; break\n",
        "    if sd is None and all(torch.is_tensor(v) for v in ckpt.values()):\n",
        "        sd = ckpt\n",
        "else:\n",
        "    raise RuntimeError(\"Unexpected checkpoint format.\")\n",
        "first = next(iter(sd))\n",
        "if first.startswith(\"module.\"):\n",
        "    sd = {k.replace(\"module.\",\"\",1): v for k,v in sd.items()}\n",
        "\n",
        "def detect_arch(keys):\n",
        "    ks = list(keys)\n",
        "    if any(k.startswith(\"enc1\") or \".enc1.\" in k for k in ks): return \"pix2pix\"\n",
        "    if any(k.startswith(\"initial_down\") or \".initial_down.\" in k for k in ks): return \"cyc_unet\"\n",
        "    if any(\".down1.\" in k for k in ks): return \"cyc_unet\"\n",
        "    return \"unknown\"\n",
        "\n",
        "arch = detect_arch(sd.keys())\n",
        "print(\"Detected arch:\", arch)\n",
        "\n",
        "G = Pix2PixUNet().to(DEVICE).eval() if arch==\"pix2pix\" else CycleUNet().to(DEVICE).eval()\n",
        "G.load_state_dict(sd, strict=False)\n",
        "\n",
        "# ---- pair 10 filenames by basename ----\n",
        "def index_by_base(folder):\n",
        "    idx = {}\n",
        "    for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.bmp\",\"*.webp\"):\n",
        "        for p in glob.glob(os.path.join(folder, ext)):\n",
        "            idx[os.path.splitext(os.path.basename(p))[0]] = p\n",
        "    return idx\n",
        "\n",
        "A = index_by_base(VALA_DIR); B = index_by_base(VALB_DIR)\n",
        "common = sorted(set(A) & set(B))[:10]\n",
        "assert common, \"No matching basenames between valA and valB.\"\n",
        "\n",
        "# ---- transforms ----\n",
        "tx_in = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
        "])\n",
        "tx_01 = transforms.Compose([transforms.Resize(256), transforms.ToTensor()])\n",
        "to01 = lambda t: (t*0.5 + 0.5).clamp(0,1)\n",
        "\n",
        "# ---- generate ----\n",
        "print(\"Saving 10 triptychs to:\", OUT_DIR)\n",
        "with torch.no_grad():\n",
        "    for base in common:\n",
        "        he_img  = Image.open(A[base]).convert(\"RGB\")\n",
        "        ihc_img = Image.open(B[base]).convert(\"RGB\")\n",
        "        x = tx_in(he_img).unsqueeze(0).to(DEVICE)\n",
        "        fake = G(x)\n",
        "        fake01 = to01(fake)[0].cpu()\n",
        "        he01   = tx_01(he_img)\n",
        "        real01 = tx_01(ihc_img)\n",
        "        trip = torch.cat([he01, real01, fake01], dim=2)\n",
        "        save_image(trip,   os.path.join(OUT_DIR, f\"{base}_TRIPTYCH_e200.png\"))\n",
        "        save_image(fake01, os.path.join(OUT_DIR, f\"{base}_fakeIHC_e200.png\"))\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "K_xYqS0DbbXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the 10 saved triptychs (H&E | Real IHC | Generated IHC)\n",
        "import os, glob, math\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "\n",
        "trip_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"*_TRIPTYCH_e200.png\")))\n",
        "print(f\"Found {len(trip_paths)} triptychs in:\", OUT_DIR)\n",
        "assert trip_paths, f\"No triptych images found in {OUT_DIR}. Check that the previous generation step ran.\"\n",
        "\n",
        "cols = 5\n",
        "rows = math.ceil(len(trip_paths) / cols)\n",
        "plt.figure(figsize=(cols*4, rows*4))\n",
        "for i, p in enumerate(trip_paths[:rows*cols]):\n",
        "    img = Image.open(p).convert(\"RGB\")\n",
        "    ax = plt.subplot(rows, cols, i+1)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(os.path.basename(p), fontsize=8)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XI-pP2KCehib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HER2/pix2pix_eval/epoch_200_preview_10\"\n",
        "trip_paths = sorted(glob.glob(os.path.join(OUT_DIR, \"*_TRIPTYCH_e200.png\")))\n",
        "p = random.choice(trip_paths)  # or set p to a specific file path\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(Image.open(p).convert(\"RGB\"))\n",
        "plt.title(os.path.basename(p))\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FCOZYgE5eoIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyuV_rrC9KXb"
      },
      "source": [
        "# **CycleGAN hybrid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s00q4Lt6Eqiu"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_PATH = \"/content/drive/MyDrive/HER2/TrainValAB\"\n",
        "\n",
        "# folders (unpaired A↔B)\n",
        "TRAIN_A_DIR = os.path.join(DATA_PATH, \"trainA\")   # domain A: H&E\n",
        "TRAIN_B_DIR = os.path.join(DATA_PATH, \"trainB\")   # domain B: IHC\n",
        "VALA_DIR    = os.path.join(DATA_PATH, \"valA\")\n",
        "VALB_DIR    = os.path.join(DATA_PATH, \"valB\")\n",
        "\n",
        "# checkpoints\n",
        "CKPT_CYC_DIR = \"/content/drive/MyDrive/checkpointsCycleHybrid\"\n",
        "os.makedirs(CKPT_CYC_DIR, exist_ok=True)\n",
        "\n",
        "# hyperparams\n",
        "IMG_SIZE    = 256\n",
        "BATCH_SIZE  = 4\n",
        "EPOCHS_CYC  = 10\n",
        "LR_CYC      = 2e-4\n",
        "BETAS_CYC   = (0.5, 0.999)\n",
        "LAMBDA_CYC  = 10     # cycle-consistency weight\n",
        "LAMBDA_ID   = 5      # identity weight (often 0.5 * LAMBDA_CYC)\n",
        "\n",
        "# quick sanity print\n",
        "for d in [TRAIN_A_DIR, TRAIN_B_DIR, VALA_DIR, VALB_DIR]:\n",
        "    print(f\"{os.path.basename(d):6s}:\", \"OK\" if os.path.exists(d) else \"MISSING\")\n",
        "print(f\"Checkpoints → {CKPT_CYC_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lIGGI4ezO1d"
      },
      "outputs": [],
      "source": [
        "# CycleGAN (hybrid) — Cell 3: models (U-Net generators + PatchGAN discriminators)\n",
        "\n",
        "# --- small conv block (down/upsample with optional InstanceNorm) ---\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_bn=True):\n",
        "        super().__init__()\n",
        "        norm = nn.InstanceNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down else\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            norm,\n",
        "            nn.ReLU(inplace=True) if act == \"relu\" else nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# --- U-Net-like generator (used for A→B and B→A) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
        "        super().__init__()\n",
        "        # down (7 steps to 1×1 bottleneck)\n",
        "        self.initial_down = ConvBlock(in_channels, features, down=True,  act=\"leaky\", use_bn=False)\n",
        "        self.down1 = ConvBlock(features,     features*2, down=True,  act=\"leaky\")\n",
        "        self.down2 = ConvBlock(features*2,   features*4, down=True,  act=\"leaky\")\n",
        "        self.down3 = ConvBlock(features*4,   features*8, down=True,  act=\"leaky\")\n",
        "        self.down4 = ConvBlock(features*8,   features*8, down=True,  act=\"leaky\")\n",
        "        self.down5 = ConvBlock(features*8,   features*8, down=True,  act=\"leaky\")\n",
        "        self.down6 = ConvBlock(features*8,   features*8, down=True,  act=\"leaky\")\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features*8, features*8, 4, 2, 1, padding_mode=\"reflect\"),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        # up + skips (mirror)\n",
        "        self.up1 = ConvBlock(features*8,     features*8,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up2 = ConvBlock(features*16,    features*8,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up3 = ConvBlock(features*16,    features*8,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up4 = ConvBlock(features*16,    features*8,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up5 = ConvBlock(features*16,    features*4,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up6 = ConvBlock(features*8,     features*2,   down=False, act=\"relu\", use_bn=True)\n",
        "        self.up7 = ConvBlock(features*4,     features,     down=False, act=\"relu\", use_bn=True)\n",
        "        self.final_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features*2, out_channels, 4, 2, 1),\n",
        "            nn.Tanh(),  # [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.initial_down(x)\n",
        "        d2 = self.down1(d1)\n",
        "        d3 = self.down2(d2)\n",
        "        d4 = self.down3(d3)\n",
        "        d5 = self.down4(d4)\n",
        "        d6 = self.down5(d5)\n",
        "        d7 = self.down6(d6)\n",
        "        b  = self.bottleneck(d7)\n",
        "\n",
        "        u1 = self.up1(b)\n",
        "        u2 = self.up2(torch.cat([u1, d7], 1))\n",
        "        u3 = self.up3(torch.cat([u2, d6], 1))\n",
        "        u4 = self.up4(torch.cat([u3, d5], 1))\n",
        "        u5 = self.up5(torch.cat([u4, d4], 1))\n",
        "        u6 = self.up6(torch.cat([u5, d3], 1))\n",
        "        u7 = self.up7(torch.cat([u6, d2], 1))\n",
        "        return self.final_up(torch.cat([u7, d1], 1))\n",
        "\n",
        "# --- 70×70 PatchGAN discriminator (spectral norm, logits output) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=(64, 128, 256, 512)):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_channels, features[0], 4, 2, 1, padding_mode=\"reflect\")),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        layers = []\n",
        "        in_ch = features[0]\n",
        "        for f in features[1:]:\n",
        "            layers += [\n",
        "                nn.utils.spectral_norm(nn.Conv2d(in_ch, f, 4, 2, 1, bias=False, padding_mode=\"reflect\")),\n",
        "                nn.InstanceNorm2d(f),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ]\n",
        "            in_ch = f\n",
        "        layers += [nn.utils.spectral_norm(nn.Conv2d(in_ch, 1, 4, 1, 1, padding_mode=\"reflect\"))]  # no Sigmoid\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        return self.model(x)  # logits map\n",
        "\n",
        "# --- weight init (DCGAN-style) ---\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "            if m.weight is not None: nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            if m.bias is not None:   nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
        "            if m.weight is not None: nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "            if m.bias is not None:   nn.init.zeros_(m.bias)\n",
        "\n",
        "# --- optional: VGG19 perceptual loss (feature L1) ---\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features[:36].eval().to(device)  # up to relu5_1\n",
        "        for p in vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.vgg = vgg\n",
        "        self.l1  = nn.L1Loss()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # [-1,1] → [0,1] for VGG\n",
        "        x01 = (x * 0.5 + 0.5).clamp(0, 1)\n",
        "        y01 = (y * 0.5 + 0.5).clamp(0, 1)\n",
        "        return self.l1(self.vgg(x01), self.vgg(y01))\n",
        "\n",
        "print(\"CycleGAN (hybrid) models ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CycleGAN — build unpaired loaders (A=H&E, B=IHC)\n",
        "class UnpairedFolderDataset(Dataset):\n",
        "    def __init__(self, root_a, root_b, size=IMG_SIZE, augment=True):\n",
        "        self.dir_a, self.dir_b = root_a, root_b\n",
        "        exts = ('.jpg', '.jpeg', '.png')\n",
        "        self.a_files = sorted([f for f in os.listdir(self.dir_a) if f.lower().endswith(exts)])\n",
        "        self.b_files = sorted([f for f in os.listdir(self.dir_b) if f.lower().endswith(exts)])\n",
        "        self.n_a, self.n_b = len(self.a_files), len(self.b_files)\n",
        "        self.n = max(self.n_a, self.n_b)  # unpaired → cycle the shorter side\n",
        "\n",
        "        ops = [transforms.Resize((IMG_SIZE, IMG_SIZE))]\n",
        "        if augment: ops.insert(0, transforms.RandomHorizontalFlip(p=0.5))\n",
        "        ops += [transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)]\n",
        "        self.tf = transforms.Compose(ops)\n",
        "\n",
        "    def __len__(self): return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        a_path = os.path.join(self.dir_a, self.a_files[idx % self.n_a])\n",
        "        b_path = os.path.join(self.dir_b, self.b_files[idx % self.n_b])\n",
        "        a = Image.open(a_path).convert('RGB')\n",
        "        b = Image.open(b_path).convert('RGB')\n",
        "        return self.tf(a), self.tf(b)\n",
        "\n",
        "# train / val loaders\n",
        "cyc_train      = UnpairedFolderDataset(TRAIN_A_DIR, TRAIN_B_DIR, size=IMG_SIZE, augment=True)\n",
        "cyc_loader     = DataLoader(cyc_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "cyc_val        = UnpairedFolderDataset(VALA_DIR, VALB_DIR, size=IMG_SIZE, augment=False)\n",
        "cyc_val_loader = DataLoader(cyc_val, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Unpaired train — A:{cyc_train.n_a}  B:{cyc_train.n_b}  |  batches:{len(cyc_loader)}\")\n"
      ],
      "metadata": {
        "id": "vFzZzbzz6mOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4EfVZfGzcKB"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Training Loop for CycleGAN (WGAN-GP + cycle + identity + perceptual, AMP, replay buffer)\n",
        "\n",
        "# --- Replay buffer ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, images):\n",
        "        out = []\n",
        "        for image in images.detach():\n",
        "            image = torch.unsqueeze(image, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(image); out.append(image)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    out.append(self.data[i].clone()); self.data[i] = image\n",
        "                else:\n",
        "                    out.append(image)\n",
        "        return torch.cat(out)\n",
        "\n",
        "# --- WGAN-GP losses ---\n",
        "def discriminator_loss_wgan_gp(disc_real_pred, disc_fake_pred, real_img, fake_img, discriminator, lambda_gp, device):\n",
        "    alpha = torch.rand(real_img.size(0), 1, 1, 1, device=device)\n",
        "    interpolates = (alpha * real_img + (1 - alpha) * fake_img).requires_grad_(True)\n",
        "    disc_interpolates = discriminator(interpolates)\n",
        "    gradients = autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(disc_interpolates, device=device),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n",
        "    return disc_fake_pred.mean() - disc_real_pred.mean() + gradient_penalty\n",
        "\n",
        "def generator_loss_wgan_gp(disc_fake_pred):\n",
        "    return -disc_fake_pred.mean()\n",
        "\n",
        "# --- models ---\n",
        "gen_A  = Generator(in_channels=3, out_channels=3, features=64).to(DEVICE)  # A→B\n",
        "gen_B  = Generator(in_channels=3, out_channels=3, features=64).to(DEVICE)  # B→A\n",
        "disc_A = Discriminator(in_channels=3).to(DEVICE)\n",
        "disc_B = Discriminator(in_channels=3).to(DEVICE)\n",
        "\n",
        "initialize_weights(gen_A); initialize_weights(gen_B)\n",
        "initialize_weights(disc_A); initialize_weights(disc_B)\n",
        "\n",
        "# --- losses ---\n",
        "criterion_Cycle      = nn.L1Loss()\n",
        "criterion_Identity   = nn.L1Loss()\n",
        "criterion_Perceptual = PerceptualLoss(DEVICE)\n",
        "\n",
        "# --- optimizers ---\n",
        "optimizer_gen    = optim.Adam(list(gen_A.parameters()) + list(gen_B.parameters()), lr=LEARNING_RATE_GEN, betas=(0.5, 0.999))\n",
        "optimizer_disc_A = optim.Adam(disc_A.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
        "optimizer_disc_B = optim.Adam(disc_B.parameters(), lr=LEARNING_RATE_DISC, betas=(0.5, 0.999))\n",
        "\n",
        "# --- schedulers (linear decay) ---\n",
        "def lambda_rule(epoch):\n",
        "    return 1.0 - max(0, epoch - DECAY_EPOCH_START) / float(NUM_EPOCHS - DECAY_EPOCH_START)\n",
        "\n",
        "scheduler_gen    = torch.optim.lr_scheduler.LambdaLR(optimizer_gen,    lr_lambda=lambda_rule)\n",
        "scheduler_disc_A = torch.optim.lr_scheduler.LambdaLR(optimizer_disc_A, lr_lambda=lambda_rule)\n",
        "scheduler_disc_B = torch.optim.lr_scheduler.LambdaLR(optimizer_disc_B, lr_lambda=lambda_rule)\n",
        "\n",
        "# --- misc ---\n",
        "start_epoch       = 0\n",
        "scaler            = GradScaler(init_scale=2.**10)\n",
        "fake_A_buffer     = ReplayBuffer()\n",
        "fake_B_buffer     = ReplayBuffer()\n",
        "\n",
        "# --- resume (optional) ---\n",
        "if LOAD_MODEL:\n",
        "    try:\n",
        "        checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, \"checkpoint_latest.pth\"), map_location=DEVICE)\n",
        "        gen_A.load_state_dict(checkpoint['gen_A_state_dict'])\n",
        "        gen_B.load_state_dict(checkpoint['gen_B_state_dict'])\n",
        "        disc_A.load_state_dict(checkpoint['disc_A_state_dict'])\n",
        "        disc_B.load_state_dict(checkpoint['disc_B_state_dict'])\n",
        "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
        "        optimizer_disc_A.load_state_dict(checkpoint['optimizer_disc_A_state_dict'])\n",
        "        optimizer_disc_B.load_state_dict(checkpoint['optimizer_disc_B_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print(f\"Resuming from epoch {start_epoch + 1}/{NUM_EPOCHS}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No latest checkpoint found. Starting from scratch.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {e}\\nStarting from scratch.\")\n",
        "\n",
        "# --- train ---\n",
        "print(\"\\nStarting CycleGAN training...\")\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    gen_A.train(); gen_B.train(); disc_A.train(); disc_B.train()\n",
        "    loop = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    avg_gen_loss_total = avg_disc_A_loss = avg_disc_B_loss = 0.0\n",
        "    avg_perceptual_loss_A = avg_perceptual_loss_B = 0.0\n",
        "    avg_gen_adv_A = avg_gen_adv_B = 0.0\n",
        "    avg_cycle_A = avg_cycle_B = 0.0\n",
        "    avg_identity_A = avg_identity_B = 0.0\n",
        "\n",
        "    for idx, (real_A, real_B) in enumerate(loop):\n",
        "        real_A = real_A.to(DEVICE)\n",
        "        real_B = real_B.to(DEVICE)\n",
        "\n",
        "        # --- D_B ---\n",
        "        optimizer_disc_B.zero_grad()\n",
        "        with autocast(DEVICE):\n",
        "            fake_B = gen_A(real_A)\n",
        "            disc_real_B_pred = disc_B(real_B)\n",
        "            fake_B_from_buffer = fake_B_buffer.push_and_pop(fake_B)\n",
        "            disc_fake_B_pred = disc_B(fake_B_from_buffer.detach())\n",
        "            loss_disc_B_adv = disc_fake_B_pred.mean() - disc_real_B_pred.mean()\n",
        "\n",
        "        scaler.scale(loss_disc_B_adv).backward(retain_graph=True)\n",
        "        scaler.unscale_(optimizer_disc_B)\n",
        "        loss_disc_B_gp = discriminator_loss_wgan_gp(\n",
        "            disc_real_B_pred, disc_fake_B_pred, real_B, fake_B_from_buffer, disc_B, LAMBDA_GP, DEVICE\n",
        "        )\n",
        "        scaler.scale(loss_disc_B_gp).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(disc_B.parameters(), max_norm=5.0)\n",
        "        scaler.step(optimizer_disc_B); scaler.update()\n",
        "        loss_disc_B_total = loss_disc_B_adv.item() + loss_disc_B_gp.item()\n",
        "\n",
        "        # --- D_A ---\n",
        "        optimizer_disc_A.zero_grad()\n",
        "        with autocast(DEVICE):\n",
        "            fake_A = gen_B(real_B)\n",
        "            disc_real_A_pred = disc_A(real_A)\n",
        "            fake_A_from_buffer = fake_A_buffer.push_and_pop(fake_A)\n",
        "            disc_fake_A_pred = disc_A(fake_A_from_buffer.detach())\n",
        "            loss_disc_A_adv = disc_fake_A_pred.mean() - disc_real_A_pred.mean()\n",
        "\n",
        "        scaler.scale(loss_disc_A_adv).backward(retain_graph=True)\n",
        "        scaler.unscale_(optimizer_disc_A)\n",
        "        loss_disc_A_gp = discriminator_loss_wgan_gp(\n",
        "            disc_real_A_pred, disc_fake_A_pred, real_A, fake_A_from_buffer, disc_A, LAMBDA_GP, DEVICE\n",
        "        )\n",
        "        scaler.scale(loss_disc_A_gp).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(disc_A.parameters(), max_norm=5.0)\n",
        "        scaler.step(optimizer_disc_A); scaler.update()\n",
        "        loss_disc_A_total = loss_disc_A_adv.item() + loss_disc_A_gp.item()\n",
        "\n",
        "        # --- Generators ---\n",
        "        optimizer_gen.zero_grad()\n",
        "        with autocast(DEVICE):\n",
        "            fake_B = gen_A(real_A); fake_A = gen_B(real_B)\n",
        "            loss_gen_adv_A_val = generator_loss_wgan_gp(disc_B(fake_B))\n",
        "            loss_gen_adv_B_val = generator_loss_wgan_gp(disc_A(fake_A))\n",
        "            cycled_A = gen_B(fake_B); loss_cycle_A_val = criterion_Cycle(cycled_A, real_A)\n",
        "            cycled_B = gen_A(fake_A); loss_cycle_B_val = criterion_Cycle(cycled_B, real_B)\n",
        "            identity_B = gen_A(real_B); loss_identity_B_val = criterion_Identity(identity_B, real_B)\n",
        "            identity_A = gen_B(real_A); loss_identity_A_val = criterion_Identity(identity_A, real_A)\n",
        "            loss_perceptual_A_val = criterion_Perceptual(fake_B.float(), real_B)\n",
        "            loss_perceptual_B_val = criterion_Perceptual(fake_A.float(), real_A)\n",
        "            loss_gen_total = (\n",
        "                loss_gen_adv_A_val + loss_gen_adv_B_val\n",
        "                + LAMBDA_CYCLE * loss_cycle_A_val + LAMBDA_CYCLE * loss_cycle_B_val\n",
        "                + LAMBDA_IDENTITY * loss_identity_A_val + LAMBDA_IDENTITY * loss_identity_B_val\n",
        "                + LAMBDA_PERCEPTUAL * loss_perceptual_A_val + LAMBDA_PERCEPTUAL * loss_perceptual_B_val\n",
        "            )\n",
        "\n",
        "        scaler.scale(loss_gen_total).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(gen_A.parameters()) + list(gen_B.parameters()), max_norm=5.0)\n",
        "        scaler.step(optimizer_gen); scaler.update()\n",
        "\n",
        "        # --- logs (running averages) ---\n",
        "        avg_disc_A_loss        += loss_disc_A_total\n",
        "        avg_disc_B_loss        += loss_disc_B_total\n",
        "        avg_gen_loss_total     += loss_gen_total.item()\n",
        "        avg_gen_adv_A          += loss_gen_adv_A_val.item()\n",
        "        avg_gen_adv_B          += loss_gen_adv_B_val.item()\n",
        "        avg_cycle_A            += loss_cycle_A_val.item()\n",
        "        avg_cycle_B            += loss_cycle_B_val.item()\n",
        "        avg_identity_A         += loss_identity_A_val.item()\n",
        "        avg_identity_B         += loss_identity_B_val.item()\n",
        "        avg_perceptual_loss_A  += loss_perceptual_A_val.item()\n",
        "        avg_perceptual_loss_B  += loss_perceptual_B_val.item()\n",
        "\n",
        "        loop.set_postfix(\n",
        "            D_A_loss=loss_disc_A_total, D_B_loss=loss_disc_B_total,\n",
        "            G_total_loss=loss_gen_total.item(),\n",
        "            G_adv_A=loss_gen_adv_A_val.item(), G_adv_B=loss_gen_adv_B_val.item(),\n",
        "            Cycle_A=loss_cycle_A_val.item(), Cycle_B=loss_cycle_B_val.item(),\n",
        "            Identity_A=loss_identity_A_val.item(), Identity_B=loss_identity_B_val.item(),\n",
        "            Percept_A=loss_perceptual_A_val.item(), Percept_B=loss_perceptual_B_val.item(),\n",
        "        )\n",
        "\n",
        "    # --- epoch end ---\n",
        "    avg_disc_A_loss       /= len(train_loader)\n",
        "    avg_disc_B_loss       /= len(train_loader)\n",
        "    avg_gen_loss_total    /= len(train_loader)\n",
        "    avg_gen_adv_A         /= len(train_loader)\n",
        "    avg_gen_adv_B         /= len(train_loader)\n",
        "    avg_cycle_A           /= len(train_loader)\n",
        "    avg_cycle_B           /= len(train_loader)\n",
        "    avg_identity_A        /= len(train_loader)\n",
        "    avg_identity_B        /= len(train_loader)\n",
        "    avg_perceptual_loss_A /= len(train_loader)\n",
        "    avg_perceptual_loss_B /= len(train_loader)\n",
        "\n",
        "    scheduler_gen.step(); scheduler_disc_A.step(); scheduler_disc_B.step()\n",
        "\n",
        "    with open(TRAINING_LOG_PATH, 'a') as f:\n",
        "        if epoch == start_epoch or (epoch == 0 and os.path.getsize(TRAINING_LOG_PATH) == 0):\n",
        "            f.write(\"Epoch,Avg_D_A_Loss,Avg_D_B_Loss,Avg_G_Total_Loss,Avg_G_Adv_A,Avg_G_Adv_B,Avg_Cycle_A,Avg_Cycle_B,Avg_Identity_A,Avg_Identity_B,Avg_Percept_A,Avg_Percept_B\\n\")\n",
        "        f.write(f\"{epoch+1},{avg_disc_A_loss:.6f},{avg_disc_B_loss:.6f},{avg_gen_loss_total:.6f},{avg_gen_adv_A:.6f},{avg_gen_adv_B:.6f},{avg_cycle_A:.6f},{avg_cycle_B:.6f},{avg_identity_A:.6f},{avg_identity_B:.6f},{avg_perceptual_loss_A:.6f},{avg_perceptual_loss_B:.6f}\\n\")\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch == start_epoch:\n",
        "        if epoch == start_epoch:\n",
        "            print(\"\\n\" + \"=\"*160)\n",
        "            print(f\"{'Epoch':<8} | {'D_A Loss':<10} | {'D_B Loss':<10} | {'G_Total Loss':<14} | {'G_Adv_A':<10} | {'G_Adv_B':<10} | {'Cycle_A':<10} | {'Cycle_B':<10} | {'Identity_A':<12} | {'Identity_B':<12} | {'Percept_A':<11} | {'Percept_B':<11}\")\n",
        "            print(\"=\"*160)\n",
        "        print(f\"{epoch+1:<8} | {avg_disc_A_loss:<10.4f} | {avg_disc_B_loss:<10.4f} | {avg_gen_loss_total:<14.4f} | {avg_gen_adv_A:<10.4f} | {avg_gen_adv_B:<10.4f} | {avg_cycle_A:<10.4f} | {avg_cycle_B:<10.4f} | {avg_identity_A:<12.4f} | {avg_identity_B:<12.4f} | {avg_perceptual_loss_A:<11.4f} | {avg_perceptual_loss_B:<11.4f}\")\n",
        "\n",
        "    # --- save ---\n",
        "    if SAVE_MODEL and (epoch + 1) % 10 == 0:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'gen_A_state_dict': gen_A.state_dict(),\n",
        "            'gen_B_state_dict': gen_B.state_dict(),\n",
        "            'disc_A_state_dict': disc_A.state_dict(),\n",
        "            'disc_B_state_dict': disc_B.state_dict(),\n",
        "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
        "            'optimizer_disc_A_state_dict': optimizer_disc_A.state_dict(),\n",
        "            'optimizer_disc_B_state_dict': optimizer_disc_B.state_dict(),\n",
        "            'scheduler_gen_state_dict': scheduler_gen.state_dict(),\n",
        "            'scheduler_disc_A_state_dict': scheduler_disc_A.state_dict(),\n",
        "            'scheduler_disc_B_state_dict': scheduler_disc_B.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
        "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, \"checkpoint_latest.pth\"))\n",
        "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
        "\n",
        "    # one sample per epoch\n",
        "    gen_A.eval(); gen_B.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (val_A, val_B) in enumerate(val_loader):\n",
        "            if i >= 1: break\n",
        "            val_A = val_A.to(DEVICE); val_B = val_B.to(DEVICE)\n",
        "            with autocast(DEVICE):\n",
        "                fake_B_val   = gen_A(val_A);  cycled_A_val = gen_B(fake_B_val)\n",
        "                fake_A_val   = gen_B(val_B);  cycled_B_val = gen_A(fake_A_val)\n",
        "            val_A_den = (val_A * 0.5 + 0.5).clamp(0, 1)\n",
        "            val_B_den = (val_B * 0.5 + 0.5).clamp(0, 1)\n",
        "            save_image(val_A_den,          os.path.join(SAMPLE_IMAGES_DIR, f\"real_A_he_sample.png\"))\n",
        "            save_image(val_B_den,          os.path.join(SAMPLE_IMAGES_DIR, f\"real_B_ihc_sample.png\"))\n",
        "            save_image((fake_B_val*0.5+0.5).clamp(0,1), os.path.join(SAMPLE_IMAGES_DIR, f\"generated_B_ihc_epoch_{epoch+1}.png\"))\n",
        "            save_image((cycled_A_val*0.5+0.5).clamp(0,1), os.path.join(SAMPLE_IMAGES_DIR, f\"cycled_A_he_epoch_{epoch+1}.png\"))\n",
        "            save_image((fake_A_val*0.5+0.5).clamp(0,1), os.path.join(SAMPLE_IMAGES_DIR, f\"generated_A_he_epoch_{epoch+1}.png\"))\n",
        "            save_image((cycled_B_val*0.5+0.5).clamp(0,1), os.path.join(SAMPLE_IMAGES_DIR, f\"cycled_B_ihc_epoch_{epoch+1}.png\"))\n",
        "    gen_A.train(); gen_B.train()\n",
        "\n",
        "print(\"\\nCycleGAN training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H1: metrics libs + imports =====\n",
        "!pip -q install torch-fidelity==0.3.0 lpips==0.1.4 piq==0.8.0\n",
        "\n",
        "import os, json, random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch_fidelity import calculate_metrics  # FID/KID\n",
        "import lpips                                   # LPIPS\n",
        "import piq                                     # SSIM/PSNR\n",
        "\n",
        "print(\"Ready on:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "09ww0LlGY7KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H3: Eval config & pairing =====\n",
        "# Use the same val split as vanilla\n",
        "VAL_DIR_A = '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "VAL_DIR_B = '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "EVAL_TAG = \"hybrid_epoch_200\"\n",
        "OUT_ROOT = \"/content/drive/MyDrive/HER2/hybrid_eval\"\n",
        "GEN_DIR  = Path(OUT_ROOT) / f\"eval_{EVAL_TAG}_A2B\"      # generated A->B\n",
        "LOG_DIR  = Path(OUT_ROOT) / f\"metrics_{EVAL_TAG}\"       # metrics\n",
        "GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 256  # keep consistent with training\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n",
        "    return sorted([p for p in Path(folder).rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "paths_A = list_images(VAL_DIR_A)\n",
        "paths_B = list_images(VAL_DIR_B)\n",
        "bname_to_B = {Path(p).stem: p for p in paths_B}\n",
        "pairs = [(str(pA), bname_to_B[Path(pA).stem]) for pA in paths_A if Path(pA).stem in bname_to_B]\n",
        "\n",
        "print(f\"A images: {len(paths_A)} | B images: {len(paths_B)} | Paired matches: {len(pairs)}\")\n",
        "\n",
        "# Transforms\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,)*3, (0.5,)*3),\n",
        "])\n",
        "to_pil = T.Compose([T.Lambda(lambda x: (x * 0.5 + 0.5).clamp(0,1)), T.ToPILImage()])\n",
        "\n",
        "# For LPIPS/SSIM/PSNR (both sides same size)\n",
        "resize_01 = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
        "    T.ToTensor(),  # [0,1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "RxeS0KX7ZNNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H4: Generate A->B for all valA =====\n",
        "with torch.no_grad():\n",
        "    for pA in paths_A:\n",
        "        out_path = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "        if out_path.exists():\n",
        "            continue\n",
        "        imgA = Image.open(pA).convert('RGB')\n",
        "        x = to_tensor(imgA).unsqueeze(0).to(DEVICE)\n",
        "        y = gen_A_h(x)[0].cpu()            # [-1,1]\n",
        "        to_pil(y).save(out_path)\n",
        "\n",
        "print(f\"Saved {len(list(GEN_DIR.glob('*.png')))} generated images to {GEN_DIR}\")\n"
      ],
      "metadata": {
        "id": "2y1ttrC3dAHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H5: FID & KID =====\n",
        "fidkid = calculate_metrics(\n",
        "    input1=str(GEN_DIR),\n",
        "    input2=str(VAL_DIR_B),\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, fid=True, kid=True, prc=False, verbose=False\n",
        ")\n",
        "with open(LOG_DIR / \"fid_kid.json\", \"w\") as f:\n",
        "    json.dump(fidkid, f, indent=2)\n",
        "print(json.dumps(fidkid, indent=2))\n"
      ],
      "metadata": {
        "id": "0mQXl4r8dGSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H6: LPIPS / SSIM / PSNR on paired matches =====\n",
        "lpips_fn = lpips.LPIPS(net='vgg').to(DEVICE).eval()\n",
        "\n",
        "rows = []\n",
        "with torch.no_grad():\n",
        "    for pA, pB in pairs:\n",
        "        gen_path = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "        if not gen_path.exists():\n",
        "            continue\n",
        "\n",
        "        # Resize both sides to the same eval size (256)\n",
        "        G01 = resize_01(Image.open(gen_path).convert('RGB')).to(DEVICE).unsqueeze(0)\n",
        "        B01 = resize_01(Image.open(pB).convert('RGB')).to(DEVICE).unsqueeze(0)\n",
        "\n",
        "        # LPIPS expects [-1,1]\n",
        "        Gm1p1, Bm1p1 = G01*2-1, B01*2-1\n",
        "        lp = lpips_fn(Gm1p1, Bm1p1).item()\n",
        "\n",
        "        ssim = piq.ssim(G01, B01, data_range=1.0).item()\n",
        "        psnr = piq.psnr(G01, B01, data_range=1.0).item()\n",
        "\n",
        "        rows.append({\"basename\": Path(pA).stem, \"lpips\": lp, \"ssim\": ssim, \"psnr\": psnr})\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"basename\")\n",
        "df.to_csv(LOG_DIR / \"paired_metrics.csv\", index=False)\n",
        "\n",
        "summary = {\n",
        "    \"tag\": EVAL_TAG,\n",
        "    \"N_pairs\": int(len(df)),\n",
        "    \"LPIPS_mean\": float(df.lpips.mean()),\n",
        "    \"LPIPS_std\": float(df.lpips.std(ddof=0)),\n",
        "    \"SSIM_mean\": float(df.ssim.mean()),\n",
        "    \"SSIM_std\": float(df.ssim.std(ddof=0)),\n",
        "    \"PSNR_mean\": float(df.psnr.mean()),\n",
        "    \"PSNR_std\": float(df.psnr.std(ddof=0)),\n",
        "}\n",
        "with open(LOG_DIR / \"paired_metrics_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n"
      ],
      "metadata": {
        "id": "AJt0PBeKdKpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell H7: Triplet grid for the thesis =====\n",
        "import matplotlib.pyplot as plt\n",
        "random.seed(42)\n",
        "\n",
        "N = 12  # number of triplets\n",
        "sample_pairs = pairs[:]\n",
        "random.shuffle(sample_pairs)\n",
        "sample_pairs = sample_pairs[:N]\n",
        "\n",
        "ncols, nrows = 3, N\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(9, 3*N), dpi=150)\n",
        "\n",
        "for i, (pA, pB) in enumerate(sample_pairs):\n",
        "    gen = GEN_DIR / (Path(pA).stem + \".png\")\n",
        "    A = Image.open(pA).convert('RGB')\n",
        "    B = Image.open(pB).convert('RGB')\n",
        "    G = Image.open(gen).convert('RGB')\n",
        "\n",
        "    axes[i,0].imshow(A); axes[i,0].set_title(\"H&E (A)\"); axes[i,0].axis('off')\n",
        "    axes[i,1].imshow(B); axes[i,1].set_title(\"IHC – Expected (B)\"); axes[i,1].axis('off')\n",
        "    axes[i,2].imshow(G); axes[i,2].set_title(\"IHC – Generated (A→B)\"); axes[i,2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "FIG_PATH = Path(OUT_ROOT) / f\"eval_{EVAL_TAG}_A-B-G_triplets.png\"\n",
        "plt.savefig(FIG_PATH, bbox_inches='tight')\n",
        "print(\"Saved figure:\", FIG_PATH)\n"
      ],
      "metadata": {
        "id": "WGubD7txdxfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plain Cycle GAN"
      ],
      "metadata": {
        "id": "9AfpNxuVBg37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 1: Imports and Global Configuration =====\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "\n",
        "# (Optional) Pretty loss tables\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "# --- Mount Google Drive (Colab) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Your dataset folders (pix2pix-style)\n",
        "    TRAIN_DIR_A: str = '/content/drive/MyDrive/HER2/TrainValAB/trainA'\n",
        "    TRAIN_DIR_B: str = '/content/drive/MyDrive/HER2/TrainValAB/trainB'\n",
        "    VAL_DIR_A:   str = '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "    VAL_DIR_B:   str = '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "    # Experiment output (new folder for vanilla CycleGAN)\n",
        "    EXP_ROOT: str = '/content/drive/MyDrive/HER2/cyclegan_vanilla'\n",
        "    SAVE_EVERY: int = 10      # checkpoint cadence\n",
        "    SAMPLE_EVERY: int = 10    # visualization cadence\n",
        "\n",
        "    # Training\n",
        "    EPOCHS: int = 200\n",
        "    BATCH_SIZE: int = 1\n",
        "    IMG_SIZE: int = 256\n",
        "    SEED: int = 42\n",
        "    DEVICE: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "\n",
        "    # Optimizer (vanilla CycleGAN uses LSGAN; these are standard)\n",
        "    LR: float = 2e-4\n",
        "    BETA1: float = 0.5\n",
        "    BETA2: float = 0.999\n",
        "    LR_DECAY_START: int = 100  # start linear decay here\n",
        "\n",
        "    # Loss weights\n",
        "    LAMBDA_CYCLE: float = 10.0\n",
        "    LAMBDA_ID: float = 5.0     # 0.5 * LAMBDA_CYCLE\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# Create output dirs\n",
        "exp_dir = Path(cfg.EXP_ROOT)\n",
        "ckpt_dir = exp_dir / 'checkpoints'\n",
        "samples_dir = exp_dir / 'samples'\n",
        "logs_dir = exp_dir / 'logs'\n",
        "for d in [exp_dir, ckpt_dir, samples_dir, logs_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(cfg.SEED)\n",
        "torch.manual_seed(cfg.SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(cfg.SEED)\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "print(\"Config:\", asdict(cfg))\n",
        "print(\"Outputs ->\", str(exp_dir))\n",
        "print(\"Device:\", cfg.DEVICE)\n"
      ],
      "metadata": {
        "id": "IHhqk38LJkAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 2: Dataset Class and DataLoader =====\n",
        "class UnpairedAB(Dataset):\n",
        "    \"\"\"\n",
        "    Expects four folders:\n",
        "      trainA/, trainB/, valA/, valB/\n",
        "    Unpaired by default. For visualization, tries to match by filename (if names align).\n",
        "    \"\"\"\n",
        "    def __init__(self, dir_A, dir_B, augment=False):\n",
        "        self.dir_A = Path(dir_A)\n",
        "        self.dir_B = Path(dir_B)\n",
        "        exts = ('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')\n",
        "        self.paths_A = sorted([p for p in self.dir_A.rglob('*') if p.suffix.lower() in exts])\n",
        "        self.paths_B = sorted([p for p in self.dir_B.rglob('*') if p.suffix.lower() in exts])\n",
        "        if len(self.paths_A) == 0 or len(self.paths_B) == 0:\n",
        "            raise RuntimeError(f\"No images found in:\\n{self.dir_A}\\n{self.dir_B}\")\n",
        "\n",
        "        # Basic aug (optional)\n",
        "        t = [transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=transforms.InterpolationMode.BICUBIC)]\n",
        "        if augment:\n",
        "            t.append(transforms.RandomHorizontalFlip())\n",
        "            t.append(transforms.RandomVerticalFlip())\n",
        "        t += [transforms.ToTensor(), transforms.Normalize((0.5,)*3, (0.5,)*3)]\n",
        "        self.tf = transforms.Compose(t)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Unpaired sampling\n",
        "        return max(len(self.paths_A), len(self.paths_B))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_A = self.paths_A[idx % len(self.paths_A)]\n",
        "        # Try to pick a B with the same basename; else random\n",
        "        base = path_A.stem\n",
        "        same = [p for p in self.paths_B if p.stem == base]\n",
        "        path_B = same[0] if same else self.paths_B[random.randint(0, len(self.paths_B)-1)]\n",
        "\n",
        "        img_A = Image.open(path_A).convert('RGB')\n",
        "        img_B = Image.open(path_B).convert('RGB')\n",
        "        return self.tf(img_A), self.tf(img_B), path_A.name, path_B.name\n",
        "\n",
        "# DataLoaders\n",
        "train_ds = UnpairedAB(cfg.TRAIN_DIR_A, cfg.TRAIN_DIR_B, augment=True)\n",
        "val_ds   = UnpairedAB(cfg.VAL_DIR_A,   cfg.VAL_DIR_B,   augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
        "\n",
        "# Helpers for visualization\n",
        "inv_norm = transforms.Normalize(mean=[-1,-1,-1], std=[2,2,2])\n",
        "def denorm(x):  # [-1,1] -> [0,1]\n",
        "    return (x * 0.5 + 0.5).clamp(0,1)\n"
      ],
      "metadata": {
        "id": "90f9Q7E2Nf8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 3: Model Definitions (Generators & Discriminators) =====\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3, padding=0),\n",
        "            nn.InstanceNorm2d(dim, affine=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3, padding=0),\n",
        "            nn.InstanceNorm2d(dim, affine=False),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_filters=64, n_blocks=9):\n",
        "        super().__init__()\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_c, n_filters, 7, padding=0),\n",
        "            nn.InstanceNorm2d(n_filters, affine=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        curr = n_filters\n",
        "        # Down 2x\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(curr, curr*2, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(curr*2, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            curr *= 2\n",
        "        # Res blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResnetBlock(curr)]\n",
        "        # Up 2x\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(curr, curr//2, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(curr//2, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            curr //= 2\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(curr, out_c, 7, padding=0),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_c=3, n_filters=64):\n",
        "        super().__init__()\n",
        "        def block(in_f, out_f, norm=True):\n",
        "            layers = [nn.Conv2d(in_f, out_f, 4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n",
        "            if norm: layers.insert(1, nn.InstanceNorm2d(out_f, affine=False))\n",
        "            return layers\n",
        "        self.model = nn.Sequential(\n",
        "            *block(in_c, n_filters, norm=False),\n",
        "            *block(n_filters, n_filters*2),\n",
        "            *block(n_filters*2, n_filters*4),\n",
        "            nn.Conv2d(n_filters*4, 1, 4, stride=1, padding=1),  # Patch score\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Instantiate models\n",
        "G_A2B = ResnetGenerator().to(cfg.DEVICE)\n",
        "G_B2A = ResnetGenerator().to(cfg.DEVICE)\n",
        "D_A   = PatchDiscriminator().to(cfg.DEVICE)\n",
        "D_B   = PatchDiscriminator().to(cfg.DEVICE)\n",
        "\n",
        "# Optimizers\n",
        "opt_G   = torch.optim.Adam(list(G_A2B.parameters()) + list(G_B2A.parameters()), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "opt_D_A = torch.optim.Adam(D_A.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "opt_D_B = torch.optim.Adam(D_B.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
        "\n",
        "# Schedulers: linear decay after LR_DECAY_START\n",
        "def lambda_rule(epoch):\n",
        "    if epoch < cfg.LR_DECAY_START:\n",
        "        return 1.0\n",
        "    return 1.0 - (epoch - cfg.LR_DECAY_START) / float(max(1, cfg.EPOCHS - cfg.LR_DECAY_START))\n",
        "\n",
        "sch_G   = torch.optim.lr_scheduler.LambdaLR(opt_G,   lr_lambda=lambda_rule)\n",
        "sch_D_A = torch.optim.lr_scheduler.LambdaLR(opt_D_A, lr_lambda=lambda_rule)\n",
        "sch_D_B = torch.optim.lr_scheduler.LambdaLR(opt_D_B, lr_lambda=lambda_rule)\n",
        "\n",
        "# Losses (LSGAN + L1)\n",
        "adv_criterion   = nn.MSELoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "\n",
        "# Image pools (stabilize D)\n",
        "class ImagePool:\n",
        "    def __init__(self, pool_size=50):\n",
        "        self.pool_size = pool_size\n",
        "        self.images = []\n",
        "    def query(self, images):\n",
        "        if self.pool_size == 0:\n",
        "            return images\n",
        "        out = []\n",
        "        for img in images:\n",
        "            img = img.detach()\n",
        "            if len(self.images) < self.pool_size:\n",
        "                self.images.append(img)\n",
        "                out.append(img)\n",
        "            else:\n",
        "                if random.random() > 0.5:\n",
        "                    idx = random.randint(0, self.pool_size - 1)\n",
        "                    tmp = self.images[idx].clone()\n",
        "                    self.images[idx] = img\n",
        "                    out.append(tmp)\n",
        "                else:\n",
        "                    out.append(img)\n",
        "        return torch.stack(out, dim=0)\n",
        "\n",
        "pool_A = ImagePool(50)\n",
        "pool_B = ImagePool(50)\n",
        "\n",
        "# Resume if latest exists\n",
        "start_epoch = 1\n",
        "latest = ckpt_dir / 'latest.pt'\n",
        "if latest.exists():\n",
        "    print(\"Resuming from\", latest)\n",
        "    s = torch.load(latest, map_location=cfg.DEVICE)\n",
        "    G_A2B.load_state_dict(s['G_A2B']); G_B2A.load_state_dict(s['G_B2A'])\n",
        "    D_A.load_state_dict(s['D_A']);     D_B.load_state_dict(s['D_B'])\n",
        "    opt_G.load_state_dict(s['opt_G']); opt_D_A.load_state_dict(s['opt_D_A']); opt_D_B.load_state_dict(s['opt_D_B'])\n",
        "    sch_G.load_state_dict(s['sch_G']); sch_D_A.load_state_dict(s['sch_D_A']); sch_D_B.load_state_dict(s['sch_D_B'])\n",
        "    start_epoch = s['epoch'] + 1\n",
        "    print(\"Start epoch:\", start_epoch)\n"
      ],
      "metadata": {
        "id": "HUH6gYvyNxJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 4 (REPLACED): Training Loop for CycleGAN with tqdm progress bar =====\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_checkpoint(epoch):\n",
        "    obj = {\n",
        "        'epoch': epoch,\n",
        "        'G_A2B': G_A2B.state_dict(),\n",
        "        'G_B2A': G_B2A.state_dict(),\n",
        "        'D_A': D_A.state_dict(),\n",
        "        'D_B': D_B.state_dict(),\n",
        "        'opt_G': opt_G.state_dict(),\n",
        "        'opt_D_A': opt_D_A.state_dict(),\n",
        "        'opt_D_B': opt_D_B.state_dict(),\n",
        "        'sch_G': sch_G.state_dict(),\n",
        "        'sch_D_A': sch_D_A.state_dict(),\n",
        "        'sch_D_B': sch_D_B.state_dict(),\n",
        "        'cfg': asdict(cfg),\n",
        "    }\n",
        "    path = ckpt_dir / f'epoch_{epoch:03d}.pt'\n",
        "    torch.save(obj, path)\n",
        "    torch.save(obj, ckpt_dir / 'latest.pt')\n",
        "    print(f'[Checkpoint] Saved {path}')\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_epoch(epoch):\n",
        "    G_A2B.eval(); G_B2A.eval()\n",
        "    try:\n",
        "        img_A, img_B, name_A, name_B = next(iter(val_loader))\n",
        "    except StopIteration:\n",
        "        return\n",
        "    img_A = img_A.to(cfg.DEVICE); img_B = img_B.to(cfg.DEVICE)\n",
        "    fake_B = G_A2B(img_A)\n",
        "\n",
        "    paired = (name_A[0].split('.')[0] == name_B[0].split('.')[0])\n",
        "    title_B = \"Expected (paired B)\" if paired else \"Reference (real B, unpaired)\"\n",
        "\n",
        "    grid = make_grid(torch.cat([denorm(img_A), denorm(img_B), denorm(fake_B)], dim=0), nrow=img_A.size(0))\n",
        "    out_path = samples_dir / f'epoch_{epoch:03d}.png'\n",
        "    save_image(grid, out_path)\n",
        "    print(f\"[Sample] Saved {out_path}\")\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.imshow(grid.permute(1,2,0).cpu().numpy()); plt.axis('off')\n",
        "    plt.title(f\"Epoch {epoch} — Left: Actual (A), Middle: {title_B}, Right: Generated (A→B)\")\n",
        "    plt.show()\n",
        "\n",
        "class LossBook:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "    def add(self, epoch, **kw):\n",
        "        row = {'epoch': epoch}; row.update({k: float(v) for k,v in kw.items()})\n",
        "        self.rows.append(row)\n",
        "    def table(self):\n",
        "        if pd is None: return None\n",
        "        return pd.DataFrame(self.rows)\n",
        "\n",
        "lossbook = LossBook()\n",
        "\n",
        "real_label = 1.0\n",
        "fake_label = 0.0\n",
        "\n",
        "for epoch in range(start_epoch, cfg.EPOCHS + 1):\n",
        "    G_A2B.train(); G_B2A.train(); D_A.train(); D_B.train()\n",
        "\n",
        "    sums = {k:0.0 for k in [\n",
        "        'G_total','G_adv_A2B','G_adv_B2A','cycle_A','cycle_B','id_A','id_B','D_A','D_B'\n",
        "    ]}\n",
        "    nb = 0\n",
        "\n",
        "    # tqdm progress bar over train batches\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg.EPOCHS}\", ncols=100, leave=False)\n",
        "    for imgs_A, imgs_B, _, _ in pbar:\n",
        "        imgs_A = imgs_A.to(cfg.DEVICE)\n",
        "        imgs_B = imgs_B.to(cfg.DEVICE)\n",
        "\n",
        "        # === Generators ===\n",
        "        opt_G.zero_grad()\n",
        "\n",
        "        # Identity: G_A2B(B) ≈ B, G_B2A(A) ≈ A\n",
        "        id_B = G_A2B(imgs_B)\n",
        "        loss_id_B = recon_criterion(id_B, imgs_B) * cfg.LAMBDA_ID\n",
        "        id_A = G_B2A(imgs_A)\n",
        "        loss_id_A = recon_criterion(id_A, imgs_A) * cfg.LAMBDA_ID\n",
        "\n",
        "        # GAN: A->B\n",
        "        fake_B = G_A2B(imgs_A)\n",
        "        pred_fake_B = D_B(fake_B)\n",
        "        valid_B = torch.ones_like(pred_fake_B, device=cfg.DEVICE)\n",
        "        loss_G_A2B = adv_criterion(pred_fake_B, valid_B)\n",
        "\n",
        "        # GAN: B->A\n",
        "        fake_A = G_B2A(imgs_B)\n",
        "        pred_fake_A = D_A(fake_A)\n",
        "        valid_A = torch.ones_like(pred_fake_A, device=cfg.DEVICE)\n",
        "        loss_G_B2A = adv_criterion(pred_fake_A, valid_A)\n",
        "\n",
        "        # Cycle\n",
        "        rec_A = G_B2A(fake_B)\n",
        "        rec_B = G_A2B(fake_A)\n",
        "        loss_cyc_A = recon_criterion(rec_A, imgs_A) * cfg.LAMBDA_CYCLE\n",
        "        loss_cyc_B = recon_criterion(rec_B, imgs_B) * cfg.LAMBDA_CYCLE\n",
        "\n",
        "        loss_G = loss_G_A2B + loss_G_B2A + loss_cyc_A + loss_cyc_B + loss_id_A + loss_id_B\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # === D_A ===\n",
        "        opt_D_A.zero_grad()\n",
        "        pred_real_A = D_A(imgs_A)\n",
        "        valid = torch.ones_like(pred_real_A, device=cfg.DEVICE)\n",
        "        loss_D_A_real = adv_criterion(pred_real_A, valid)\n",
        "\n",
        "        fake_A_pool = pool_A.query(fake_A)\n",
        "        pred_fake_A = D_A(fake_A_pool.detach())\n",
        "        fake = torch.zeros_like(pred_fake_A, device=cfg.DEVICE)\n",
        "        loss_D_A_fake = adv_criterion(pred_fake_A, fake)\n",
        "        loss_DA = 0.5*(loss_D_A_real + loss_D_A_fake)\n",
        "        loss_DA.backward()\n",
        "        opt_D_A.step()\n",
        "\n",
        "        # === D_B ===\n",
        "        opt_D_B.zero_grad()\n",
        "        pred_real_B = D_B(imgs_B)\n",
        "        valid = torch.ones_like(pred_real_B, device=cfg.DEVICE)\n",
        "        loss_D_B_real = adv_criterion(pred_real_B, valid)\n",
        "\n",
        "        fake_B_pool = pool_B.query(fake_B)\n",
        "        pred_fake_B = D_B(fake_B_pool.detach())\n",
        "        fake = torch.zeros_like(pred_fake_B, device=cfg.DEVICE)\n",
        "        loss_D_B_fake = adv_criterion(pred_fake_B, fake)\n",
        "        loss_DB = 0.5*(loss_D_B_real + loss_D_B_fake)\n",
        "        loss_DB.backward()\n",
        "        opt_D_B.step()\n",
        "\n",
        "        # Accumulate + update progress bar postfix\n",
        "        sums['G_total']    += loss_G.item()\n",
        "        sums['G_adv_A2B']  += loss_G_A2B.item()\n",
        "        sums['G_adv_B2A']  += loss_G_B2A.item()\n",
        "        sums['cycle_A']    += loss_cyc_A.item()\n",
        "        sums['cycle_B']    += loss_cyc_B.item()\n",
        "        sums['id_A']       += loss_id_A.item()\n",
        "        sums['id_B']       += loss_id_B.item()\n",
        "        sums['D_A']        += loss_DA.item()\n",
        "        sums['D_B']        += loss_DB.item()\n",
        "        nb += 1\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"G\": f\"{sums['G_total']/nb:.3f}\",\n",
        "            \"D_A\": f\"{sums['D_A']/nb:.3f}\",\n",
        "            \"D_B\": f\"{sums['D_B']/nb:.3f}\",\n",
        "        })\n",
        "\n",
        "    # Step schedulers once per epoch\n",
        "    sch_G.step(); sch_D_A.step(); sch_D_B.step()\n",
        "\n",
        "    means = {k: v/max(1,nb) for k,v in sums.items()}\n",
        "    lossbook.add(epoch, **means)\n",
        "\n",
        "    # End-of-epoch summary line\n",
        "    print(f\"Epoch {epoch:03d}/{cfg.EPOCHS} | \"\n",
        "          f\"G:{means['G_total']:.4f} | D_A:{means['D_A']:.4f} D_B:{means['D_B']:.4f} | \"\n",
        "          f\"cycA:{means['cycle_A']:.3f} cycB:{means['cycle_B']:.3f} | idA:{means['id_A']:.3f} idB:{means['id_B']:.3f}\")\n",
        "\n",
        "    # Sample + table every SAMPLE_EVERY\n",
        "    if epoch % cfg.SAMPLE_EVERY == 0:\n",
        "        visualize_epoch(epoch)\n",
        "        if pd is not None:\n",
        "            df = lossbook.table()\n",
        "            print(df.to_string(index=False))\n",
        "        else:\n",
        "            print(\"[Info] Install pandas for a formatted loss table.\")\n",
        "\n",
        "    # Checkpoint every SAVE_EVERY (and on final epoch)\n",
        "    if epoch % cfg.SAVE_EVERY == 0 or epoch == cfg.EPOCHS:\n",
        "        save_checkpoint(epoch)\n",
        "\n",
        "# Save final CSV and show final sample\n",
        "if pd is not None:\n",
        "    df = lossbook.table()\n",
        "    csv_path = logs_dir / 'epoch_losses.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"[Log] Saved loss CSV to {csv_path}\")\n",
        "\n",
        "visualize_epoch(epoch=cfg.EPOCHS)\n"
      ],
      "metadata": {
        "id": "mY0uyXUXN92D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5: Install metrics libs (run once per fresh Colab) =====\n",
        "!pip -q install torch-fidelity==0.3.0 lpips==0.1.4 piq==0.8.0\n"
      ],
      "metadata": {
        "id": "2poIe2kOOBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_fidelity import calculate_metrics"
      ],
      "metadata": {
        "id": "gwDRH8SJHcgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 6: Eval config, pairing, helpers =====\n",
        "import os, glob, random, json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch_fidelity import calculate_metrics  # torch-fidelity\n",
        "import lpips  # perceptual\n",
        "import piq    # SSIM/PSNR\n",
        "\n",
        "# --- Choose the split you want to evaluate (val or test) ---\n",
        "EVAL_DIR_A = cfg.VAL_DIR_A  # '/content/drive/MyDrive/HER2/TrainValAB/valA'\n",
        "EVAL_DIR_B = cfg.VAL_DIR_B  # '/content/drive/MyDrive/HER2/TrainValAB/valB'\n",
        "\n",
        "# --- Where to dump generated images & results for this checkpoint ---\n",
        "EVAL_TAG       = \"epoch_150\"  # <-- change to the checkpoint you want to evaluate\n",
        "GEN_OUT_DIR    = samples_dir / f\"eval_{EVAL_TAG}_A2B\"\n",
        "METRICS_OUTDIR = logs_dir / f\"metrics_{EVAL_TAG}\"\n",
        "GEN_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "METRICS_OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Transforms to feed the net, and to convert back to PIL for saving\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), interpolation=T.InterpolationMode.BICUBIC),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,)*3, (0.5,)*3)\n",
        "])\n",
        "to_pil = T.Compose([T.Lambda(lambda x: (x * 0.5 + 0.5).clamp(0,1)), T.ToPILImage()])\n",
        "\n",
        "# Pair files by basename (only for LPIPS/SSIM/PSNR)\n",
        "def list_images(folder):\n",
        "    exts = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n",
        "    return sorted([p for p in Path(folder).rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "paths_A = list_images(EVAL_DIR_A)\n",
        "paths_B = list_images(EVAL_DIR_B)\n",
        "bname_to_B = {Path(p).stem: p for p in paths_B}\n",
        "pairs = []\n",
        "for pA in paths_A:\n",
        "    bn = Path(pA).stem\n",
        "    if bn in bname_to_B:\n",
        "        pairs.append((str(pA), bname_to_B[bn]))\n",
        "\n",
        "print(f\"Found {len(paths_A)} A images, {len(paths_B)} B images, and {len(pairs)} paired matches by filename.\")\n"
      ],
      "metadata": {
        "id": "6KrYspntHGzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 7: Load checkpoint & export A→B translations =====\n",
        "# Load the checkpoint you want to evaluate\n",
        "ckpt = torch.load(ckpt_dir / f\"{EVAL_TAG}.pt\", map_location=cfg.DEVICE)\n",
        "G_A2B.load_state_dict(ckpt['G_A2B'])\n",
        "G_B2A.load_state_dict(ckpt['G_B2A'])\n",
        "G_A2B.eval(); G_B2A.eval()\n",
        "\n",
        "# Generate A->B for all A images in EVAL_DIR_A\n",
        "with torch.no_grad():\n",
        "    for pA in paths_A:\n",
        "        imgA = Image.open(pA).convert('RGB')\n",
        "        x = to_tensor(imgA).unsqueeze(0).to(cfg.DEVICE)\n",
        "        y_fake = G_A2B(x)[0].cpu()\n",
        "        pil = to_pil(y_fake)\n",
        "        out_path = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "        pil.save(out_path)\n",
        "\n",
        "print(f\"Saved {len(paths_A)} generated images to {GEN_OUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "fFreP7U0HLHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 8: Compute FID & KID =====\n",
        "# torch-fidelity expects directories with images\n",
        "metrics = calculate_metrics(\n",
        "    input1=str(GEN_OUT_DIR),\n",
        "    input2=str(EVAL_DIR_B),\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    isc=False, fid=True, kid=True, prc=False, verbose=False\n",
        ")\n",
        "with open(METRICS_OUTDIR / \"fid_kid.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(json.dumps(metrics, indent=2))\n"
      ],
      "metadata": {
        "id": "MsseFNoAH8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 9 (REPLACED): LPIPS, SSIM, PSNR on paired matches =====\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import lpips\n",
        "import piq\n",
        "\n",
        "# set eval size = training size used by the nets\n",
        "EVAL_SIZE = cfg.IMG_SIZE\n",
        "\n",
        "# transforms\n",
        "to_unit = T.ToTensor()  # [0,1]\n",
        "resize_01 = T.Compose([\n",
        "    T.Resize((EVAL_SIZE, EVAL_SIZE), interpolation=T.InterpolationMode.BICUBIC, antialias=True),\n",
        "    T.ToTensor(),  # [0,1]\n",
        "])\n",
        "def to_m1p1(x01):  # [0,1] -> [-1,1]\n",
        "    return x01 * 2 - 1\n",
        "\n",
        "lpips_fn = lpips.LPIPS(net='vgg').to(cfg.DEVICE).eval()\n",
        "\n",
        "rows = []\n",
        "with torch.no_grad():\n",
        "    for pA, pB in pairs:\n",
        "        gen_path = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "        if not gen_path.exists():\n",
        "            continue\n",
        "\n",
        "        # Load & resize both to the SAME size\n",
        "        Gimg01 = resize_01(Image.open(gen_path).convert('RGB')).to(cfg.DEVICE).unsqueeze(0)  # [1,3,H,W], 0..1\n",
        "        Bimg01 = resize_01(Image.open(pB).convert('RGB')).to(cfg.DEVICE).unsqueeze(0)\n",
        "\n",
        "        # LPIPS expects [-1,1]\n",
        "        Gm1p1 = to_m1p1(Gimg01)\n",
        "        Bm1p1 = to_m1p1(Bimg01)\n",
        "        lp = lpips_fn(Gm1p1, Bm1p1).item()\n",
        "\n",
        "        # SSIM / PSNR on [0,1]\n",
        "        ssim_val = piq.ssim(Gimg01, Bimg01, data_range=1.0).item()\n",
        "        psnr_val = piq.psnr(Gimg01, Bimg01, data_range=1.0).item()\n",
        "\n",
        "        rows.append({\n",
        "            \"basename\": Path(pA).stem,\n",
        "            \"lpips\": lp,\n",
        "            \"ssim\": ssim_val,\n",
        "            \"psnr\": psnr_val\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"basename\")\n",
        "df.to_csv(METRICS_OUTDIR / \"paired_metrics.csv\", index=False)\n",
        "\n",
        "agg = {\n",
        "    \"N_pairs\": int(len(df)),\n",
        "    \"LPIPS_mean\": float(df[\"lpips\"].mean()),\n",
        "    \"LPIPS_std\": float(df[\"lpips\"].std(ddof=0)),\n",
        "    \"SSIM_mean\": float(df[\"ssim\"].mean()),\n",
        "    \"SSIM_std\": float(df[\"ssim\"].std(ddof=0)),\n",
        "    \"PSNR_mean\": float(df[\"psnr\"].mean()),\n",
        "    \"PSNR_std\": float(df[\"psnr\"].std(ddof=0)),\n",
        "}\n",
        "with open(METRICS_OUTDIR / \"paired_metrics_summary.json\", \"w\") as f:\n",
        "    json.dump(agg, f, indent=2)\n",
        "\n",
        "print(agg)\n"
      ],
      "metadata": {
        "id": "yIu5nqrcItq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 10: Create a grid of examples for the thesis =====\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "N = 12  # how many triplets to show\n",
        "sample_pairs = pairs[:]\n",
        "random.shuffle(sample_pairs)\n",
        "sample_pairs = sample_pairs[:N]\n",
        "\n",
        "ncols = 3\n",
        "nrows = N\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(9, 3*N), dpi=150)\n",
        "\n",
        "for i, (pA, pB) in enumerate(sample_pairs):\n",
        "    gen = GEN_OUT_DIR / (Path(pA).stem + \".png\")\n",
        "    A = Image.open(pA).convert('RGB')\n",
        "    B = Image.open(pB).convert('RGB')\n",
        "    G = Image.open(gen).convert('RGB')\n",
        "\n",
        "    axes[i,0].imshow(A); axes[i,0].set_title(\"H&E (A)\"); axes[i,0].axis('off')\n",
        "    axes[i,1].imshow(B); axes[i,1].set_title(\"IHC – Expected (B)\"); axes[i,1].axis('off')\n",
        "    axes[i,2].imshow(G); axes[i,2].set_title(\"IHC – Generated (A→B)\"); axes[i,2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig_path = samples_dir / f\"eval_{EVAL_TAG}_A-B-G_triplets.png\"\n",
        "plt.savefig(fig_path, bbox_inches='tight')\n",
        "print(f\"Saved figure: {fig_path}\")\n"
      ],
      "metadata": {
        "id": "aCmD3HCQJE-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "_V7J6tENfcCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Comparison charts: Our model, Pix2Pix, CycleGAN Hybrid, CycleGAN Vanilla (e150), Paper (CycleGAN, ASP) ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "# ----- Fill in your numbers (MIST–HER2) -----\n",
        "# If \"Our model\" should be something else, just edit the first row.\n",
        "rows = [\n",
        "    (\"Our model (CycleGAN vanilla)\", 110,  75.16, 0.1380, 14.55),   # <-- our model (edit if needed)\n",
        "    (\"Pix2pix (U-Net + PatchGAN)\",   200, 325.08, 0.0765, 10.33),\n",
        "    (\"CycleGAN (hybrid)\",            200,  93.30, 0.1208, 13.71),\n",
        "    (\"CycleGAN (vanilla)\",           150,  69.24, 0.1400, 14.52),\n",
        "    # Paper (Li et al., MICCAI'23) on MIST–HER2\n",
        "    (\"CycleGAN (paper, MIST–HER2)\", \"paper\", 240.30, 0.6386, np.nan),  # PSNR not reported\n",
        "    (\"ASP (paper, MIST–HER2)\",      \"paper\",  51.40, 0.4881, np.nan),  # PSNR not reported\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"Model\",\"Epoch\",\"FID\",\"SSIM\",\"PSNR (dB)\"])\n",
        "\n",
        "# --- Output folder & CSV\n",
        "outdir = Path(\"charts_comparison\"); outdir.mkdir(exist_ok=True)\n",
        "(df.assign(Epoch=df[\"Epoch\"].astype(str))\n",
        "   .to_csv(outdir / \"comparison_table.csv\", index=False))\n",
        "\n",
        "# --- Axis labels like \"Model (e150)\" or \"(paper)\"\n",
        "def epoch_tag(e):\n",
        "    try: return f\"e{int(e)}\"\n",
        "    except: return str(e)\n",
        "\n",
        "labels = df.apply(lambda r: f\"{r['Model']} ({epoch_tag(r['Epoch'])})\", axis=1)\n",
        "\n",
        "# --- Colors\n",
        "palette = {\n",
        "    \"Our model (CycleGAN vanilla) (e110)\": \"#1f77b4\",  # blue\n",
        "    \"Pix2pix (U-Net + PatchGAN) (e200)\":   \"#ff7f0e\",  # orange\n",
        "    \"CycleGAN (hybrid) (e200)\":            \"#2ca02c\",  # green\n",
        "    \"CycleGAN (vanilla) (e150)\":           \"#d62728\",  # red\n",
        "    \"CycleGAN (paper, MIST–HER2) (paper)\": \"#9467bd\",  # purple\n",
        "    \"ASP (paper, MIST–HER2) (paper)\":      \"#8c564b\",  # brown\n",
        "}\n",
        "colors = [palette.get(lbl, \"#888888\") for lbl in labels]\n",
        "\n",
        "def plot_metric(metric, lower_is_better=False, title_prefix=\"MIST–HER2\"):\n",
        "    x = np.arange(len(labels))\n",
        "    vals = df[metric].astype(float).to_numpy()\n",
        "    finite_vals = vals[np.isfinite(vals)]\n",
        "    ymax = float(finite_vals.max()) * 1.25 if finite_vals.size else 1.0\n",
        "\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.bar(x, vals, color=colors)\n",
        "    plt.xticks(x, labels, rotation=20, ha=\"right\")\n",
        "    plt.ylabel(metric)\n",
        "    title_dir = \"(lower is better)\" if lower_is_better else \"(higher is better)\"\n",
        "    plt.title(f\"{title_prefix}: {metric} by model {title_dir}\")\n",
        "    plt.ylim(0, ymax)\n",
        "\n",
        "    # annotate numeric bars\n",
        "    for xi, v in zip(x, vals):\n",
        "        if np.isfinite(v):\n",
        "            plt.text(xi, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9, rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fname = f\"{title_prefix.replace(' ','_')}_{metric.replace(' ','_').replace('(','').replace(')','')}.png\"\n",
        "    plt.savefig(outdir / fname, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outdir / fname)\n",
        "\n",
        "# --- Build the three charts\n",
        "plot_metric(\"FID\", lower_is_better=True)\n",
        "plot_metric(\"SSIM\", lower_is_better=False)\n",
        "plot_metric(\"PSNR (dB)\", lower_is_better=False)\n",
        "\n",
        "print(\"CSV saved to:\", (outdir / \"comparison_table.csv\").resolve())\n",
        "print(\"Charts saved in:\", outdir.resolve())\n"
      ],
      "metadata": {
        "id": "juh8T2uALsr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FT5vNnN9XGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO/MeGtomY0KhXmE9b55TpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}